{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Resize, Compose, v2\n",
    "from torchvision.datasets import Cityscapes\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msvenbbs\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate = 1e-3\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "subsize = (128, 256)\n",
    "momentum = 0.9\n",
    "mean, std = ([0.2776, 0.3131, 0.2740]), ([0.1695, 0.1745, 0.1706])\n",
    "weights = torch.tensor([3.0492e-04, 7.7058e-04, 1.6697e-03, 1.0166e-03, 1.5975e-03, 8.6582e-04,\n",
    "        3.7707e-05, 2.2719e-04, 1.2423e-03, 1.9831e-03, 6.5811e-05, 1.0036e-03,\n",
    "        1.0501e-03, 2.6375e-03, 1.8904e-03, 2.3288e-03, 1.7155e-03, 2.7163e-03,\n",
    "        1.9230e-03, 1.2427e-03, 9.0243e-05, 9.4631e-04, 3.6473e-04, 1.3061e-03,\n",
    "        3.7688e-03, 2.2016e-04, 4.6955e-03, 5.7737e-03, 3.3058e-02, 2.2757e-02,\n",
    "        4.5719e-03, 1.4638e-02, 5.8080e-03, 8.7571e-01]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:28: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\Sven\\AppData\\Local\\Temp\\ipykernel_2132\\958200182.py:28: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  normal_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=normal_transform, target_transform=normal_target_transform)\n",
      "C:\\Users\\Sven\\AppData\\Local\\Temp\\ipykernel_2132\\958200182.py:29: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  augmented_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=augmented_transform, target_transform=augmented_target_transform)\n",
      "c:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "augmented_transform = transforms.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize(subsize),\n",
    "    v2.RandomHorizontalFlip(1),  # Random horizontal flip\n",
    "    #v2.RandomRotation(10),  # Random rotation up to 10 degrees\n",
    "    #v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jitter\n",
    "    \n",
    "])\n",
    "\n",
    "augmented_target_transform = transforms.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize(subsize),\n",
    "    v2.RandomHorizontalFlip(1),  # Same transformation as input images\n",
    "    #v2.RandomRotation(10),  # Same transformation as input images\n",
    "])\n",
    "\n",
    "normal_transform = transforms.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize(subsize),\n",
    "])\n",
    "normal_target_transform = transforms.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize(subsize),\n",
    "])\n",
    "\n",
    "\n",
    "#Desktop\n",
    "normal_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=normal_transform, target_transform=normal_target_transform)\n",
    "augmented_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=augmented_transform, target_transform=augmented_target_transform)\n",
    "dataset = torch.utils.data.ConcatDataset([normal_dataset, augmented_dataset])\n",
    "\n",
    "\n",
    "#Laptop\n",
    "#dataset = Cityscapes(root=\"C:/Users/20182573/Documents/CityScapes\", split='train', mode='fine', target_type='semantic', transform=transform, target_transform=target_transform)\n",
    "\n",
    "#subset_small, subset_big = random_split(dataset, [0.2,0.8])\n",
    "train_dataset, val_dataset = random_split(dataset, [0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_std(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "\n",
    "    mean /= len(loader.dataset)\n",
    "    std /= len(loader.dataset)\n",
    "    return mean, std\n",
    "\n",
    "#mean, std = mean_and_std(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model import Model\n",
    "model = Model().cuda()\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights, ignore_index=255)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, run):\n",
    "    \"\"\"\n",
    "    Train a model for 1 epoch.\n",
    "\n",
    "    Params:\n",
    "    - dataloader:   dataset to train on.\n",
    "    - model:        the model object to be trained.\n",
    "    - loss_fn:      the loss function.\n",
    "    - optimizer:    the desired optimization.\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() #Set the model to train mode\n",
    "    for batch, (IMG,SEGM) in enumerate(dataloader):\n",
    "        IMG = IMG.to('cuda')\n",
    "        SEGM  = (SEGM*255).long().squeeze()     #*255 because the id are normalized between 0-1\n",
    "        SEGM = utils.map_id_to_train_id(SEGM).to('cuda')\n",
    "        \n",
    "        #predict\n",
    "        pred = model(IMG)\n",
    "        #Loss\n",
    "        loss = loss_fn(pred, SEGM)\n",
    "        \n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #print loss during training\n",
    "        loss, current = loss.item(), (batch + 1) * len(IMG)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        run.log({\"train_loss\": loss})\n",
    "    \n",
    "    return run\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Test a model.\n",
    "\n",
    "    Params:\n",
    "    - dataloader:   dataset to test on.\n",
    "    - model:        the model object to be tested.\n",
    "    - loss_fn:      the loss function.\n",
    "    \"\"\"\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() #model in eval mode\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (IMG,SEGM) in enumerate(dataloader):\n",
    "            IMG = IMG.to('cuda')\n",
    "            SEGM  = (SEGM*255).long().squeeze()     #*255 because the id are normalized between 0-1\n",
    "            SEGM = utils.map_id_to_train_id(SEGM).to('cuda')\n",
    "\n",
    "            pred = model(IMG)\n",
    "            test_loss += loss_fn(pred, SEGM).item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Sven\\Documents\\GitHub\\CS_challenge\\wandb\\run-20240327_113603-xehmpzn7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/svenbbs/CS_challenge/runs/xehmpzn7' target=\"_blank\">1.8</a></strong> to <a href='https://wandb.ai/svenbbs/CS_challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/svenbbs/CS_challenge' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/svenbbs/CS_challenge/runs/xehmpzn7' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge/runs/xehmpzn7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.893908  [   64/ 4760]\n",
      "loss: 3.390970  [  128/ 4760]\n",
      "loss: 3.091957  [  192/ 4760]\n",
      "loss: 2.915618  [  256/ 4760]\n",
      "loss: 2.831128  [  320/ 4760]\n",
      "loss: 2.516648  [  384/ 4760]\n",
      "loss: 2.464610  [  448/ 4760]\n",
      "loss: 2.217815  [  512/ 4760]\n",
      "loss: 2.189900  [  576/ 4760]\n",
      "loss: 2.147421  [  640/ 4760]\n",
      "loss: 1.906577  [  704/ 4760]\n",
      "loss: 1.846121  [  768/ 4760]\n",
      "loss: 1.852178  [  832/ 4760]\n",
      "loss: 1.810483  [  896/ 4760]\n",
      "loss: 1.707912  [  960/ 4760]\n",
      "loss: 1.657291  [ 1024/ 4760]\n",
      "loss: 1.579134  [ 1088/ 4760]\n",
      "loss: 1.623006  [ 1152/ 4760]\n",
      "loss: 1.465245  [ 1216/ 4760]\n",
      "loss: 1.451856  [ 1280/ 4760]\n",
      "loss: 1.398741  [ 1344/ 4760]\n",
      "loss: 1.469832  [ 1408/ 4760]\n",
      "loss: 1.426301  [ 1472/ 4760]\n",
      "loss: 1.446106  [ 1536/ 4760]\n",
      "loss: 1.356265  [ 1600/ 4760]\n",
      "loss: 1.285599  [ 1664/ 4760]\n",
      "loss: 1.275582  [ 1728/ 4760]\n",
      "loss: 1.247824  [ 1792/ 4760]\n",
      "loss: 1.278619  [ 1856/ 4760]\n",
      "loss: 1.271701  [ 1920/ 4760]\n",
      "loss: 1.297851  [ 1984/ 4760]\n",
      "loss: 1.281780  [ 2048/ 4760]\n",
      "loss: 1.190276  [ 2112/ 4760]\n",
      "loss: 1.162301  [ 2176/ 4760]\n",
      "loss: 1.183545  [ 2240/ 4760]\n",
      "loss: 1.042725  [ 2304/ 4760]\n",
      "loss: 1.183380  [ 2368/ 4760]\n",
      "loss: 1.052318  [ 2432/ 4760]\n",
      "loss: 1.119453  [ 2496/ 4760]\n",
      "loss: 1.094302  [ 2560/ 4760]\n",
      "loss: 1.097508  [ 2624/ 4760]\n",
      "loss: 1.069760  [ 2688/ 4760]\n",
      "loss: 1.023568  [ 2752/ 4760]\n",
      "loss: 1.031523  [ 2816/ 4760]\n",
      "loss: 0.958705  [ 2880/ 4760]\n",
      "loss: 1.053907  [ 2944/ 4760]\n",
      "loss: 1.083827  [ 3008/ 4760]\n",
      "loss: 1.127424  [ 3072/ 4760]\n",
      "loss: 1.055741  [ 3136/ 4760]\n",
      "loss: 1.026797  [ 3200/ 4760]\n",
      "loss: 0.900629  [ 3264/ 4760]\n",
      "loss: 0.919751  [ 3328/ 4760]\n",
      "loss: 0.937244  [ 3392/ 4760]\n",
      "loss: 0.906832  [ 3456/ 4760]\n",
      "loss: 0.904181  [ 3520/ 4760]\n",
      "loss: 0.957107  [ 3584/ 4760]\n",
      "loss: 0.871042  [ 3648/ 4760]\n",
      "loss: 0.981134  [ 3712/ 4760]\n",
      "loss: 0.894051  [ 3776/ 4760]\n",
      "loss: 0.873300  [ 3840/ 4760]\n",
      "loss: 0.917724  [ 3904/ 4760]\n",
      "loss: 0.862254  [ 3968/ 4760]\n",
      "loss: 0.871657  [ 4032/ 4760]\n",
      "loss: 0.870914  [ 4096/ 4760]\n",
      "loss: 0.862955  [ 4160/ 4760]\n",
      "loss: 0.852224  [ 4224/ 4760]\n",
      "loss: 0.822247  [ 4288/ 4760]\n",
      "loss: 0.843925  [ 4352/ 4760]\n",
      "loss: 0.912515  [ 4416/ 4760]\n",
      "loss: 0.848935  [ 4480/ 4760]\n",
      "loss: 0.824743  [ 4544/ 4760]\n",
      "loss: 0.885367  [ 4608/ 4760]\n",
      "loss: 0.811235  [ 4672/ 4760]\n",
      "loss: 0.777662  [ 4736/ 4760]\n",
      "loss: 0.832507  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 0.919910 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.760776  [   64/ 4760]\n",
      "loss: 0.797418  [  128/ 4760]\n",
      "loss: 0.786236  [  192/ 4760]\n",
      "loss: 0.794060  [  256/ 4760]\n",
      "loss: 0.860688  [  320/ 4760]\n",
      "loss: 0.857354  [  384/ 4760]\n",
      "loss: 0.776050  [  448/ 4760]\n",
      "loss: 0.881523  [  512/ 4760]\n",
      "loss: 0.806361  [  576/ 4760]\n",
      "loss: 0.763620  [  640/ 4760]\n",
      "loss: 0.785201  [  704/ 4760]\n",
      "loss: 0.882843  [  768/ 4760]\n",
      "loss: 0.804138  [  832/ 4760]\n",
      "loss: 0.810786  [  896/ 4760]\n",
      "loss: 0.858255  [  960/ 4760]\n",
      "loss: 0.831515  [ 1024/ 4760]\n",
      "loss: 0.729784  [ 1088/ 4760]\n",
      "loss: 0.782339  [ 1152/ 4760]\n",
      "loss: 0.827667  [ 1216/ 4760]\n",
      "loss: 0.780764  [ 1280/ 4760]\n",
      "loss: 0.777210  [ 1344/ 4760]\n",
      "loss: 0.820394  [ 1408/ 4760]\n",
      "loss: 0.791454  [ 1472/ 4760]\n",
      "loss: 0.820155  [ 1536/ 4760]\n",
      "loss: 0.785991  [ 1600/ 4760]\n",
      "loss: 0.744118  [ 1664/ 4760]\n",
      "loss: 0.777808  [ 1728/ 4760]\n",
      "loss: 0.768399  [ 1792/ 4760]\n",
      "loss: 0.713799  [ 1856/ 4760]\n",
      "loss: 0.812584  [ 1920/ 4760]\n",
      "loss: 0.697778  [ 1984/ 4760]\n",
      "loss: 0.728746  [ 2048/ 4760]\n",
      "loss: 0.801979  [ 2112/ 4760]\n",
      "loss: 0.774798  [ 2176/ 4760]\n",
      "loss: 0.759042  [ 2240/ 4760]\n",
      "loss: 0.687362  [ 2304/ 4760]\n",
      "loss: 0.725778  [ 2368/ 4760]\n",
      "loss: 0.690164  [ 2432/ 4760]\n",
      "loss: 0.696930  [ 2496/ 4760]\n",
      "loss: 0.701690  [ 2560/ 4760]\n",
      "loss: 0.721484  [ 2624/ 4760]\n",
      "loss: 0.772632  [ 2688/ 4760]\n",
      "loss: 0.793686  [ 2752/ 4760]\n",
      "loss: 0.681007  [ 2816/ 4760]\n",
      "loss: 0.716516  [ 2880/ 4760]\n",
      "loss: 0.685907  [ 2944/ 4760]\n",
      "loss: 0.693733  [ 3008/ 4760]\n",
      "loss: 0.659484  [ 3072/ 4760]\n",
      "loss: 0.778172  [ 3136/ 4760]\n",
      "loss: 0.684164  [ 3200/ 4760]\n",
      "loss: 0.638777  [ 3264/ 4760]\n",
      "loss: 0.691943  [ 3328/ 4760]\n",
      "loss: 0.691657  [ 3392/ 4760]\n",
      "loss: 0.677806  [ 3456/ 4760]\n",
      "loss: 0.694820  [ 3520/ 4760]\n",
      "loss: 0.655464  [ 3584/ 4760]\n",
      "loss: 0.660894  [ 3648/ 4760]\n",
      "loss: 0.632113  [ 3712/ 4760]\n",
      "loss: 0.654800  [ 3776/ 4760]\n",
      "loss: 0.676885  [ 3840/ 4760]\n",
      "loss: 0.661410  [ 3904/ 4760]\n",
      "loss: 0.665890  [ 3968/ 4760]\n",
      "loss: 0.633401  [ 4032/ 4760]\n",
      "loss: 0.733007  [ 4096/ 4760]\n",
      "loss: 0.639471  [ 4160/ 4760]\n",
      "loss: 0.615218  [ 4224/ 4760]\n",
      "loss: 0.687173  [ 4288/ 4760]\n",
      "loss: 0.674405  [ 4352/ 4760]\n",
      "loss: 0.698807  [ 4416/ 4760]\n",
      "loss: 0.668412  [ 4480/ 4760]\n",
      "loss: 0.675630  [ 4544/ 4760]\n",
      "loss: 0.659457  [ 4608/ 4760]\n",
      "loss: 0.748487  [ 4672/ 4760]\n",
      "loss: 0.709251  [ 4736/ 4760]\n",
      "loss: 0.705822  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 0.658716 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.642695  [   64/ 4760]\n",
      "loss: 0.683243  [  128/ 4760]\n",
      "loss: 0.646025  [  192/ 4760]\n",
      "loss: 0.700533  [  256/ 4760]\n",
      "loss: 0.660423  [  320/ 4760]\n",
      "loss: 0.747659  [  384/ 4760]\n",
      "loss: 0.668639  [  448/ 4760]\n",
      "loss: 0.692900  [  512/ 4760]\n",
      "loss: 0.612367  [  576/ 4760]\n",
      "loss: 0.621360  [  640/ 4760]\n",
      "loss: 0.662002  [  704/ 4760]\n",
      "loss: 0.624491  [  768/ 4760]\n",
      "loss: 0.593459  [  832/ 4760]\n",
      "loss: 0.673337  [  896/ 4760]\n",
      "loss: 0.642966  [  960/ 4760]\n",
      "loss: 0.711016  [ 1024/ 4760]\n",
      "loss: 0.625224  [ 1088/ 4760]\n",
      "loss: 0.630964  [ 1152/ 4760]\n",
      "loss: 0.684839  [ 1216/ 4760]\n",
      "loss: 0.576345  [ 1280/ 4760]\n",
      "loss: 0.584930  [ 1344/ 4760]\n",
      "loss: 0.623342  [ 1408/ 4760]\n",
      "loss: 0.656379  [ 1472/ 4760]\n",
      "loss: 0.660892  [ 1536/ 4760]\n",
      "loss: 0.688559  [ 1600/ 4760]\n",
      "loss: 0.578368  [ 1664/ 4760]\n",
      "loss: 0.584644  [ 1728/ 4760]\n",
      "loss: 0.629118  [ 1792/ 4760]\n",
      "loss: 0.629090  [ 1856/ 4760]\n",
      "loss: 0.630504  [ 1920/ 4760]\n",
      "loss: 0.611106  [ 1984/ 4760]\n",
      "loss: 0.612521  [ 2048/ 4760]\n",
      "loss: 0.629602  [ 2112/ 4760]\n",
      "loss: 0.624191  [ 2176/ 4760]\n",
      "loss: 0.663593  [ 2240/ 4760]\n",
      "loss: 0.631823  [ 2304/ 4760]\n",
      "loss: 0.675636  [ 2368/ 4760]\n",
      "loss: 0.654154  [ 2432/ 4760]\n",
      "loss: 0.595801  [ 2496/ 4760]\n",
      "loss: 0.619460  [ 2560/ 4760]\n",
      "loss: 0.652405  [ 2624/ 4760]\n",
      "loss: 0.568872  [ 2688/ 4760]\n",
      "loss: 0.615445  [ 2752/ 4760]\n",
      "loss: 0.693391  [ 2816/ 4760]\n",
      "loss: 0.578907  [ 2880/ 4760]\n",
      "loss: 0.569489  [ 2944/ 4760]\n",
      "loss: 0.641882  [ 3008/ 4760]\n",
      "loss: 0.572035  [ 3072/ 4760]\n",
      "loss: 0.622347  [ 3136/ 4760]\n",
      "loss: 0.657234  [ 3200/ 4760]\n",
      "loss: 0.640790  [ 3264/ 4760]\n",
      "loss: 0.612391  [ 3328/ 4760]\n",
      "loss: 0.617306  [ 3392/ 4760]\n",
      "loss: 0.597833  [ 3456/ 4760]\n",
      "loss: 0.613822  [ 3520/ 4760]\n",
      "loss: 0.559152  [ 3584/ 4760]\n",
      "loss: 0.552722  [ 3648/ 4760]\n",
      "loss: 0.587695  [ 3712/ 4760]\n",
      "loss: 0.572182  [ 3776/ 4760]\n",
      "loss: 0.550942  [ 3840/ 4760]\n",
      "loss: 0.611353  [ 3904/ 4760]\n",
      "loss: 0.613580  [ 3968/ 4760]\n",
      "loss: 0.602152  [ 4032/ 4760]\n",
      "loss: 0.545646  [ 4096/ 4760]\n",
      "loss: 0.543489  [ 4160/ 4760]\n",
      "loss: 0.548731  [ 4224/ 4760]\n",
      "loss: 0.579548  [ 4288/ 4760]\n",
      "loss: 0.645506  [ 4352/ 4760]\n",
      "loss: 0.540987  [ 4416/ 4760]\n",
      "loss: 0.571947  [ 4480/ 4760]\n",
      "loss: 0.591995  [ 4544/ 4760]\n",
      "loss: 0.540473  [ 4608/ 4760]\n",
      "loss: 0.559110  [ 4672/ 4760]\n",
      "loss: 0.631545  [ 4736/ 4760]\n",
      "loss: 0.583906  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 0.635812 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.543467  [   64/ 4760]\n",
      "loss: 0.508413  [  128/ 4760]\n",
      "loss: 0.523678  [  192/ 4760]\n",
      "loss: 0.566232  [  256/ 4760]\n",
      "loss: 0.515494  [  320/ 4760]\n",
      "loss: 0.649996  [  384/ 4760]\n",
      "loss: 0.565726  [  448/ 4760]\n",
      "loss: 0.508345  [  512/ 4760]\n",
      "loss: 0.543180  [  576/ 4760]\n",
      "loss: 0.503898  [  640/ 4760]\n",
      "loss: 0.613666  [  704/ 4760]\n",
      "loss: 0.533900  [  768/ 4760]\n",
      "loss: 0.555813  [  832/ 4760]\n",
      "loss: 0.546766  [  896/ 4760]\n",
      "loss: 0.599836  [  960/ 4760]\n",
      "loss: 0.528604  [ 1024/ 4760]\n",
      "loss: 0.594140  [ 1088/ 4760]\n",
      "loss: 0.646303  [ 1152/ 4760]\n",
      "loss: 0.545584  [ 1216/ 4760]\n",
      "loss: 0.566541  [ 1280/ 4760]\n",
      "loss: 0.504822  [ 1344/ 4760]\n",
      "loss: 0.588941  [ 1408/ 4760]\n",
      "loss: 0.557091  [ 1472/ 4760]\n",
      "loss: 0.541208  [ 1536/ 4760]\n",
      "loss: 0.598574  [ 1600/ 4760]\n",
      "loss: 0.538990  [ 1664/ 4760]\n",
      "loss: 0.555401  [ 1728/ 4760]\n",
      "loss: 0.544168  [ 1792/ 4760]\n",
      "loss: 0.571390  [ 1856/ 4760]\n",
      "loss: 0.566303  [ 1920/ 4760]\n",
      "loss: 0.506123  [ 1984/ 4760]\n",
      "loss: 0.513370  [ 2048/ 4760]\n",
      "loss: 0.509676  [ 2112/ 4760]\n",
      "loss: 0.580045  [ 2176/ 4760]\n",
      "loss: 0.542422  [ 2240/ 4760]\n",
      "loss: 0.608501  [ 2304/ 4760]\n",
      "loss: 0.596704  [ 2368/ 4760]\n",
      "loss: 0.601510  [ 2432/ 4760]\n",
      "loss: 0.552865  [ 2496/ 4760]\n",
      "loss: 0.600350  [ 2560/ 4760]\n",
      "loss: 0.520721  [ 2624/ 4760]\n",
      "loss: 0.597721  [ 2688/ 4760]\n",
      "loss: 0.539039  [ 2752/ 4760]\n",
      "loss: 0.552358  [ 2816/ 4760]\n",
      "loss: 0.546687  [ 2880/ 4760]\n",
      "loss: 0.640222  [ 2944/ 4760]\n",
      "loss: 0.562262  [ 3008/ 4760]\n",
      "loss: 0.535595  [ 3072/ 4760]\n",
      "loss: 0.541930  [ 3136/ 4760]\n",
      "loss: 0.566364  [ 3200/ 4760]\n",
      "loss: 0.687866  [ 3264/ 4760]\n",
      "loss: 0.548727  [ 3328/ 4760]\n",
      "loss: 0.599914  [ 3392/ 4760]\n",
      "loss: 0.563075  [ 3456/ 4760]\n",
      "loss: 0.624147  [ 3520/ 4760]\n",
      "loss: 0.659040  [ 3584/ 4760]\n",
      "loss: 0.579146  [ 3648/ 4760]\n",
      "loss: 0.522102  [ 3712/ 4760]\n",
      "loss: 0.595627  [ 3776/ 4760]\n",
      "loss: 0.512866  [ 3840/ 4760]\n",
      "loss: 0.548285  [ 3904/ 4760]\n",
      "loss: 0.524697  [ 3968/ 4760]\n",
      "loss: 0.533920  [ 4032/ 4760]\n",
      "loss: 0.557237  [ 4096/ 4760]\n",
      "loss: 0.526318  [ 4160/ 4760]\n",
      "loss: 0.552283  [ 4224/ 4760]\n",
      "loss: 0.586790  [ 4288/ 4760]\n",
      "loss: 0.523659  [ 4352/ 4760]\n",
      "loss: 0.562222  [ 4416/ 4760]\n",
      "loss: 0.585521  [ 4480/ 4760]\n",
      "loss: 0.541087  [ 4544/ 4760]\n",
      "loss: 0.506888  [ 4608/ 4760]\n",
      "loss: 0.579362  [ 4672/ 4760]\n",
      "loss: 0.509696  [ 4736/ 4760]\n",
      "loss: 0.517781  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 0.553045 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.554372  [   64/ 4760]\n",
      "loss: 0.544356  [  128/ 4760]\n",
      "loss: 0.579851  [  192/ 4760]\n",
      "loss: 0.480747  [  256/ 4760]\n",
      "loss: 0.508928  [  320/ 4760]\n",
      "loss: 0.525231  [  384/ 4760]\n",
      "loss: 0.513861  [  448/ 4760]\n",
      "loss: 0.521401  [  512/ 4760]\n",
      "loss: 0.517369  [  576/ 4760]\n",
      "loss: 0.536920  [  640/ 4760]\n",
      "loss: 0.518209  [  704/ 4760]\n",
      "loss: 0.585847  [  768/ 4760]\n",
      "loss: 0.535146  [  832/ 4760]\n",
      "loss: 0.519585  [  896/ 4760]\n",
      "loss: 0.537332  [  960/ 4760]\n",
      "loss: 0.506776  [ 1024/ 4760]\n",
      "loss: 0.509417  [ 1088/ 4760]\n",
      "loss: 0.480073  [ 1152/ 4760]\n",
      "loss: 0.441907  [ 1216/ 4760]\n",
      "loss: 0.579716  [ 1280/ 4760]\n",
      "loss: 0.547884  [ 1344/ 4760]\n",
      "loss: 0.572513  [ 1408/ 4760]\n",
      "loss: 0.537684  [ 1472/ 4760]\n",
      "loss: 0.554791  [ 1536/ 4760]\n",
      "loss: 0.556421  [ 1600/ 4760]\n",
      "loss: 0.519770  [ 1664/ 4760]\n",
      "loss: 0.496031  [ 1728/ 4760]\n",
      "loss: 0.479174  [ 1792/ 4760]\n",
      "loss: 0.521958  [ 1856/ 4760]\n",
      "loss: 0.529838  [ 1920/ 4760]\n",
      "loss: 0.468350  [ 1984/ 4760]\n",
      "loss: 0.505161  [ 2048/ 4760]\n",
      "loss: 0.477895  [ 2112/ 4760]\n",
      "loss: 0.519909  [ 2176/ 4760]\n",
      "loss: 0.526136  [ 2240/ 4760]\n",
      "loss: 0.466998  [ 2304/ 4760]\n",
      "loss: 0.583927  [ 2368/ 4760]\n",
      "loss: 0.514315  [ 2432/ 4760]\n",
      "loss: 0.494344  [ 2496/ 4760]\n",
      "loss: 0.523740  [ 2560/ 4760]\n",
      "loss: 0.500660  [ 2624/ 4760]\n",
      "loss: 0.439745  [ 2688/ 4760]\n",
      "loss: 0.541173  [ 2752/ 4760]\n",
      "loss: 0.507988  [ 2816/ 4760]\n",
      "loss: 0.495760  [ 2880/ 4760]\n",
      "loss: 0.510173  [ 2944/ 4760]\n",
      "loss: 0.506087  [ 3008/ 4760]\n",
      "loss: 0.505881  [ 3072/ 4760]\n",
      "loss: 0.485225  [ 3136/ 4760]\n",
      "loss: 0.503607  [ 3200/ 4760]\n",
      "loss: 0.472065  [ 3264/ 4760]\n",
      "loss: 0.502251  [ 3328/ 4760]\n",
      "loss: 0.512409  [ 3392/ 4760]\n",
      "loss: 0.443055  [ 3456/ 4760]\n",
      "loss: 0.482982  [ 3520/ 4760]\n",
      "loss: 0.477646  [ 3584/ 4760]\n",
      "loss: 0.530848  [ 3648/ 4760]\n",
      "loss: 0.511228  [ 3712/ 4760]\n",
      "loss: 0.490189  [ 3776/ 4760]\n",
      "loss: 0.523602  [ 3840/ 4760]\n",
      "loss: 0.548465  [ 3904/ 4760]\n",
      "loss: 0.549718  [ 3968/ 4760]\n",
      "loss: 0.500085  [ 4032/ 4760]\n",
      "loss: 0.514348  [ 4096/ 4760]\n",
      "loss: 0.492555  [ 4160/ 4760]\n",
      "loss: 0.556183  [ 4224/ 4760]\n",
      "loss: 0.572321  [ 4288/ 4760]\n",
      "loss: 0.554274  [ 4352/ 4760]\n",
      "loss: 0.593955  [ 4416/ 4760]\n",
      "loss: 0.487213  [ 4480/ 4760]\n",
      "loss: 0.536819  [ 4544/ 4760]\n",
      "loss: 0.485114  [ 4608/ 4760]\n",
      "loss: 0.508250  [ 4672/ 4760]\n",
      "loss: 0.545284  [ 4736/ 4760]\n",
      "loss: 0.494687  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 0.531674 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"CS_challenge\", name=\"1.8\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": {lr_rate},\n",
    "        \"epochs\": {epochs},\n",
    "        \"Momentum\": {momentum},\n",
    "        \"Batch_size\": {batch_size},\n",
    "        \"model version\": 0.0,\n",
    "        \"subset size [%]\": 100,\n",
    "        \"resize\": {subsize},\n",
    "        \"Normalized\": False,\n",
    "        \"Optimizer\": 'Adam',\n",
    "        \"weighted classes\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    run = train(train_dataloader, model, loss_fn, optimizer, run)\n",
    "    test_loss = test(val_dataloader, model, loss_fn)\n",
    "    run.log({\"Test_loss\": test_loss, \"Epoch\": t})\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Test_loss</td><td>█▃▃▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>4</td></tr><tr><td>Test_loss</td><td>0.53167</td></tr><tr><td>train_loss</td><td>0.49469</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">1.8</strong> at: <a href='https://wandb.ai/svenbbs/CS_challenge/runs/xehmpzn7' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge/runs/xehmpzn7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240327_113603-xehmpzn7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5lsm0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
