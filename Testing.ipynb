{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "from torchvision.datasets import Cityscapes\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msvenbbs\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "subsize = (128, 256)\n",
    "momentum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = Compose([\n",
    "    Resize(subsize),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "target_transform = Compose([\n",
    "    Resize(subsize),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "#Desktop\n",
    "dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=transform, target_transform=target_transform)\n",
    "\n",
    "#Laptop\n",
    "#dataset = Cityscapes(root=\"C:/Users/20182573/Documents/CityScapes\", split='train', mode='fine', target_type='semantic', transform=transform, target_transform=target_transform)\n",
    "\n",
    "#subset_small, subset_big = random_split(dataset, [0.2,0.8])\n",
    "train_dataset, val_dataset = random_split(dataset, [0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2380"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model import Model\n",
    "\n",
    "model = Model().cuda()\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, run):\n",
    "    \"\"\"\n",
    "    Train a model for 1 epoch.\n",
    "\n",
    "    Params:\n",
    "    - dataloader:   dataset to train on.\n",
    "    - model:        the model object to be trained.\n",
    "    - loss_fn:      the loss function.\n",
    "    - optimizer:    the desired optimization.\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() #Set the model to train mode\n",
    "    for batch, (IMG,SEGM) in enumerate(dataloader):\n",
    "        IMG = IMG.to('cuda')\n",
    "        SEGM  = (SEGM*255).long().squeeze()     #*255 because the id are normalized between 0-1\n",
    "        SEGM = utils.map_id_to_train_id(SEGM).to('cuda')\n",
    "        \n",
    "        #predict\n",
    "        pred = model(IMG)\n",
    "        #Loss\n",
    "        loss = loss_fn(pred, SEGM)\n",
    "        \n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #print loss during training\n",
    "        loss, current = loss.item(), (batch + 1) * len(IMG)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        run.log({\"train_loss\": loss})\n",
    "    \n",
    "    return run\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Test a model.\n",
    "\n",
    "    Params:\n",
    "    - dataloader:   dataset to test on.\n",
    "    - model:        the model object to be tested.\n",
    "    - loss_fn:      the loss function.\n",
    "    \"\"\"\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() #model in eval mode\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (IMG,SEGM) in enumerate(dataloader):\n",
    "            IMG = IMG.to('cuda')\n",
    "            SEGM  = (SEGM*255).long().squeeze()     #*255 because the id are normalized between 0-1\n",
    "            SEGM = utils.map_id_to_train_id(SEGM).to('cuda')\n",
    "\n",
    "            pred = model(IMG)\n",
    "            test_loss += loss_fn(pred, SEGM).item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Sven\\Documents\\GitHub\\CS_challenge\\wandb\\run-20240313_153128-gfdw846c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/svenbbs/CS_challenge/runs/gfdw846c' target=\"_blank\">Baseline</a></strong> to <a href='https://wandb.ai/svenbbs/CS_challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/svenbbs/CS_challenge' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/svenbbs/CS_challenge/runs/gfdw846c' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge/runs/gfdw846c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.680243  [   64/ 2380]\n",
      "loss: 3.690689  [  128/ 2380]\n",
      "loss: 3.662894  [  192/ 2380]\n",
      "loss: 3.661673  [  256/ 2380]\n",
      "loss: 3.651207  [  320/ 2380]\n",
      "loss: 3.632881  [  384/ 2380]\n",
      "loss: 3.632411  [  448/ 2380]\n",
      "loss: 3.628073  [  512/ 2380]\n",
      "loss: 3.615046  [  576/ 2380]\n",
      "loss: 3.591819  [  640/ 2380]\n",
      "loss: 3.594000  [  704/ 2380]\n",
      "loss: 3.571939  [  768/ 2380]\n",
      "loss: 3.566234  [  832/ 2380]\n",
      "loss: 3.557127  [  896/ 2380]\n",
      "loss: 3.545800  [  960/ 2380]\n",
      "loss: 3.540694  [ 1024/ 2380]\n",
      "loss: 3.525391  [ 1088/ 2380]\n",
      "loss: 3.523522  [ 1152/ 2380]\n",
      "loss: 3.508590  [ 1216/ 2380]\n",
      "loss: 3.496364  [ 1280/ 2380]\n",
      "loss: 3.484048  [ 1344/ 2380]\n",
      "loss: 3.492723  [ 1408/ 2380]\n",
      "loss: 3.474499  [ 1472/ 2380]\n",
      "loss: 3.463879  [ 1536/ 2380]\n",
      "loss: 3.444469  [ 1600/ 2380]\n",
      "loss: 3.467240  [ 1664/ 2380]\n",
      "loss: 3.455887  [ 1728/ 2380]\n",
      "loss: 3.433334  [ 1792/ 2380]\n",
      "loss: 3.444580  [ 1856/ 2380]\n",
      "loss: 3.408376  [ 1920/ 2380]\n",
      "loss: 3.418385  [ 1984/ 2380]\n",
      "loss: 3.394811  [ 2048/ 2380]\n",
      "loss: 3.397725  [ 2112/ 2380]\n",
      "loss: 3.396230  [ 2176/ 2380]\n",
      "loss: 3.394623  [ 2240/ 2380]\n",
      "loss: 3.390261  [ 2304/ 2380]\n",
      "loss: 3.387552  [ 2368/ 2380]\n",
      "loss: 3.382367  [  456/ 2380]\n",
      "Test Error: \n",
      " Avg loss: 3.359460 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.349937  [   64/ 2380]\n",
      "loss: 3.367136  [  128/ 2380]\n",
      "loss: 3.375504  [  192/ 2380]\n",
      "loss: 3.334733  [  256/ 2380]\n",
      "loss: 3.347075  [  320/ 2380]\n",
      "loss: 3.337296  [  384/ 2380]\n",
      "loss: 3.326683  [  448/ 2380]\n",
      "loss: 3.347791  [  512/ 2380]\n",
      "loss: 3.317349  [  576/ 2380]\n",
      "loss: 3.306382  [  640/ 2380]\n",
      "loss: 3.323458  [  704/ 2380]\n",
      "loss: 3.297904  [  768/ 2380]\n",
      "loss: 3.307699  [  832/ 2380]\n",
      "loss: 3.300806  [  896/ 2380]\n",
      "loss: 3.294548  [  960/ 2380]\n",
      "loss: 3.289119  [ 1024/ 2380]\n",
      "loss: 3.274013  [ 1088/ 2380]\n",
      "loss: 3.264086  [ 1152/ 2380]\n",
      "loss: 3.241903  [ 1216/ 2380]\n",
      "loss: 3.239891  [ 1280/ 2380]\n",
      "loss: 3.226304  [ 1344/ 2380]\n",
      "loss: 3.249925  [ 1408/ 2380]\n",
      "loss: 3.223456  [ 1472/ 2380]\n",
      "loss: 3.204098  [ 1536/ 2380]\n",
      "loss: 3.190720  [ 1600/ 2380]\n",
      "loss: 3.236396  [ 1664/ 2380]\n",
      "loss: 3.220031  [ 1728/ 2380]\n",
      "loss: 3.191483  [ 1792/ 2380]\n",
      "loss: 3.227522  [ 1856/ 2380]\n",
      "loss: 3.167164  [ 1920/ 2380]\n",
      "loss: 3.185457  [ 1984/ 2380]\n",
      "loss: 3.157709  [ 2048/ 2380]\n",
      "loss: 3.164370  [ 2112/ 2380]\n",
      "loss: 3.170769  [ 2176/ 2380]\n",
      "loss: 3.171015  [ 2240/ 2380]\n",
      "loss: 3.171484  [ 2304/ 2380]\n",
      "loss: 3.171010  [ 2368/ 2380]\n",
      "loss: 3.179658  [  456/ 2380]\n",
      "Test Error: \n",
      " Avg loss: 3.152550 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.133464  [   64/ 2380]\n",
      "loss: 3.151534  [  128/ 2380]\n",
      "loss: 3.179624  [  192/ 2380]\n",
      "loss: 3.112715  [  256/ 2380]\n",
      "loss: 3.134408  [  320/ 2380]\n",
      "loss: 3.128406  [  384/ 2380]\n",
      "loss: 3.110691  [  448/ 2380]\n",
      "loss: 3.140769  [  512/ 2380]\n",
      "loss: 3.095695  [  576/ 2380]\n",
      "loss: 3.096570  [  640/ 2380]\n",
      "loss: 3.119216  [  704/ 2380]\n",
      "loss: 3.090819  [  768/ 2380]\n",
      "loss: 3.108249  [  832/ 2380]\n",
      "loss: 3.100084  [  896/ 2380]\n",
      "loss: 3.095906  [  960/ 2380]\n",
      "loss: 3.091221  [ 1024/ 2380]\n",
      "loss: 3.075769  [ 1088/ 2380]\n",
      "loss: 3.056012  [ 1152/ 2380]\n",
      "loss: 3.024127  [ 1216/ 2380]\n",
      "loss: 3.031553  [ 1280/ 2380]\n",
      "loss: 3.016073  [ 1344/ 2380]\n",
      "loss: 3.050535  [ 1408/ 2380]\n",
      "loss: 3.016206  [ 1472/ 2380]\n",
      "loss: 2.988785  [ 1536/ 2380]\n",
      "loss: 2.979652  [ 1600/ 2380]\n",
      "loss: 3.044381  [ 1664/ 2380]\n",
      "loss: 3.021406  [ 1728/ 2380]\n",
      "loss: 2.988912  [ 1792/ 2380]\n",
      "loss: 3.045102  [ 1856/ 2380]\n",
      "loss: 2.964457  [ 1920/ 2380]\n",
      "loss: 2.989314  [ 1984/ 2380]\n",
      "loss: 2.959086  [ 2048/ 2380]\n",
      "loss: 2.967549  [ 2112/ 2380]\n",
      "loss: 2.979995  [ 2176/ 2380]\n",
      "loss: 2.983865  [ 2240/ 2380]\n",
      "loss: 2.987588  [ 2304/ 2380]\n",
      "loss: 2.986568  [ 2368/ 2380]\n",
      "loss: 3.003887  [  456/ 2380]\n",
      "Test Error: \n",
      " Avg loss: 2.968351 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.948663  [   64/ 2380]\n",
      "loss: 2.969048  [  128/ 2380]\n",
      "loss: 3.011310  [  192/ 2380]\n",
      "loss: 2.925978  [  256/ 2380]\n",
      "loss: 2.954268  [  320/ 2380]\n",
      "loss: 2.952396  [  384/ 2380]\n",
      "loss: 2.927289  [  448/ 2380]\n",
      "loss: 2.965931  [  512/ 2380]\n",
      "loss: 2.909431  [  576/ 2380]\n",
      "loss: 2.918830  [  640/ 2380]\n",
      "loss: 2.945964  [  704/ 2380]\n",
      "loss: 2.916906  [  768/ 2380]\n",
      "loss: 2.940003  [  832/ 2380]\n",
      "loss: 2.931649  [  896/ 2380]\n",
      "loss: 2.926977  [  960/ 2380]\n",
      "loss: 2.921594  [ 1024/ 2380]\n",
      "loss: 2.908302  [ 1088/ 2380]\n",
      "loss: 2.880224  [ 1152/ 2380]\n",
      "loss: 2.841151  [ 1216/ 2380]\n",
      "loss: 2.855674  [ 1280/ 2380]\n",
      "loss: 2.837063  [ 1344/ 2380]\n",
      "loss: 2.882983  [ 1408/ 2380]\n",
      "loss: 2.840348  [ 1472/ 2380]\n",
      "loss: 2.808250  [ 1536/ 2380]\n",
      "loss: 2.802579  [ 1600/ 2380]\n",
      "loss: 2.878305  [ 1664/ 2380]\n",
      "loss: 2.849678  [ 1728/ 2380]\n",
      "loss: 2.816537  [ 1792/ 2380]\n",
      "loss: 2.885798  [ 1856/ 2380]\n",
      "loss: 2.789953  [ 1920/ 2380]\n",
      "loss: 2.819675  [ 1984/ 2380]\n",
      "loss: 2.788827  [ 2048/ 2380]\n",
      "loss: 2.796232  [ 2112/ 2380]\n",
      "loss: 2.813271  [ 2176/ 2380]\n",
      "loss: 2.819220  [ 2240/ 2380]\n",
      "loss: 2.824980  [ 2304/ 2380]\n",
      "loss: 2.823776  [ 2368/ 2380]\n",
      "loss: 2.845841  [  456/ 2380]\n",
      "Test Error: \n",
      " Avg loss: 2.803307 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.783438  [   64/ 2380]\n",
      "loss: 2.806480  [  128/ 2380]\n",
      "loss: 2.857374  [  192/ 2380]\n",
      "loss: 2.759676  [  256/ 2380]\n",
      "loss: 2.791340  [  320/ 2380]\n",
      "loss: 2.792194  [  384/ 2380]\n",
      "loss: 2.761414  [  448/ 2380]\n",
      "loss: 2.807241  [  512/ 2380]\n",
      "loss: 2.738758  [  576/ 2380]\n",
      "loss: 2.755898  [  640/ 2380]\n",
      "loss: 2.785161  [  704/ 2380]\n",
      "loss: 2.756657  [  768/ 2380]\n",
      "loss: 2.782964  [  832/ 2380]\n",
      "loss: 2.775450  [  896/ 2380]\n",
      "loss: 2.768044  [  960/ 2380]\n",
      "loss: 2.760652  [ 1024/ 2380]\n",
      "loss: 2.750810  [ 1088/ 2380]\n",
      "loss: 2.713883  [ 1152/ 2380]\n",
      "loss: 2.670751  [ 1216/ 2380]\n",
      "loss: 2.689552  [ 1280/ 2380]\n",
      "loss: 2.666796  [ 1344/ 2380]\n",
      "loss: 2.725652  [ 1408/ 2380]\n",
      "loss: 2.672268  [ 1472/ 2380]\n",
      "loss: 2.638453  [ 1536/ 2380]\n",
      "loss: 2.635132  [ 1600/ 2380]\n",
      "loss: 2.715863  [ 1664/ 2380]\n",
      "loss: 2.682107  [ 1728/ 2380]\n",
      "loss: 2.650499  [ 1792/ 2380]\n",
      "loss: 2.727336  [ 1856/ 2380]\n",
      "loss: 2.619796  [ 1920/ 2380]\n",
      "loss: 2.653503  [ 1984/ 2380]\n",
      "loss: 2.622987  [ 2048/ 2380]\n",
      "loss: 2.627763  [ 2112/ 2380]\n",
      "loss: 2.648140  [ 2176/ 2380]\n",
      "loss: 2.653742  [ 2240/ 2380]\n",
      "loss: 2.661854  [ 2304/ 2380]\n",
      "loss: 2.661900  [ 2368/ 2380]\n",
      "loss: 2.688051  [  456/ 2380]\n",
      "Test Error: \n",
      " Avg loss: 2.643029 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.617950  [   64/ 2380]\n",
      "loss: 2.644516  [  128/ 2380]\n",
      "loss: 2.700637  [  192/ 2380]\n",
      "loss: 2.594326  [  256/ 2380]\n",
      "loss: 2.627351  [  320/ 2380]\n",
      "loss: 2.632956  [  384/ 2380]\n",
      "loss: 2.597733  [  448/ 2380]\n",
      "loss: 2.650456  [  512/ 2380]\n",
      "loss: 2.569602  [  576/ 2380]\n",
      "loss: 2.596096  [  640/ 2380]\n",
      "loss: 2.625690  [  704/ 2380]\n",
      "loss: 2.599719  [  768/ 2380]\n",
      "loss: 2.628850  [  832/ 2380]\n",
      "loss: 2.622383  [  896/ 2380]\n",
      "loss: 2.613746  [  960/ 2380]\n",
      "loss: 2.605470  [ 1024/ 2380]\n",
      "loss: 2.598382  [ 1088/ 2380]\n",
      "loss: 2.553503  [ 1152/ 2380]\n",
      "loss: 2.510271  [ 1216/ 2380]\n",
      "loss: 2.533167  [ 1280/ 2380]\n",
      "loss: 2.505610  [ 1344/ 2380]\n",
      "loss: 2.580558  [ 1408/ 2380]\n",
      "loss: 2.514720  [ 1472/ 2380]\n",
      "loss: 2.482856  [ 1536/ 2380]\n",
      "loss: 2.481714  [ 1600/ 2380]\n",
      "loss: 2.566372  [ 1664/ 2380]\n",
      "loss: 2.528221  [ 1728/ 2380]\n",
      "loss: 2.499751  [ 1792/ 2380]\n",
      "loss: 2.581907  [ 1856/ 2380]\n",
      "loss: 2.467789  [ 1920/ 2380]\n",
      "loss: 2.502925  [ 1984/ 2380]\n",
      "loss: 2.476030  [ 2048/ 2380]\n",
      "loss: 2.477202  [ 2112/ 2380]\n",
      "loss: 2.504071  [ 2176/ 2380]\n",
      "loss: 2.506714  [ 2240/ 2380]\n",
      "loss: 2.517478  [ 2304/ 2380]\n",
      "loss: 2.521412  [ 2368/ 2380]\n",
      "loss: 2.549344  [  456/ 2380]\n",
      "Test Error: \n",
      " Avg loss: 2.499793 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.474117  [   64/ 2380]\n",
      "loss: 2.505366  [  128/ 2380]\n",
      "loss: 2.565032  [  192/ 2380]\n",
      "loss: 2.451631  [  256/ 2380]\n",
      "loss: 2.487610  [  320/ 2380]\n",
      "loss: 2.500036  [  384/ 2380]\n",
      "loss: 2.459592  [  448/ 2380]\n",
      "loss: 2.517347  [  512/ 2380]\n",
      "loss: 2.428445  [  576/ 2380]\n",
      "loss: 2.463412  [  640/ 2380]\n",
      "loss: 2.492070  [  704/ 2380]\n",
      "loss: 2.469814  [  768/ 2380]\n",
      "loss: 2.502177  [  832/ 2380]\n",
      "loss: 2.495908  [  896/ 2380]\n",
      "loss: 2.485915  [  960/ 2380]\n",
      "loss: 2.476969  [ 1024/ 2380]\n",
      "loss: 2.473197  [ 1088/ 2380]\n",
      "loss: 2.421360  [ 1152/ 2380]\n",
      "loss: 2.381283  [ 1216/ 2380]\n",
      "loss: 2.404560  [ 1280/ 2380]\n",
      "loss: 2.373733  [ 1344/ 2380]\n",
      "loss: 2.461427  [ 1408/ 2380]\n",
      "loss: 2.386391  [ 1472/ 2380]\n",
      "loss: 2.356621  [ 1536/ 2380]\n",
      "loss: 2.356010  [ 1600/ 2380]\n",
      "loss: 2.444156  [ 1664/ 2380]\n",
      "loss: 2.401747  [ 1728/ 2380]\n",
      "loss: 2.375980  [ 1792/ 2380]\n",
      "loss: 2.462098  [ 1856/ 2380]\n",
      "loss: 2.345160  [ 1920/ 2380]\n",
      "loss: 2.378234  [ 1984/ 2380]\n",
      "loss: 2.357148  [ 2048/ 2380]\n",
      "loss: 2.352308  [ 2112/ 2380]\n",
      "loss: 2.388059  [ 2176/ 2380]\n",
      "loss: 2.387050  [ 2240/ 2380]\n",
      "loss: 2.398990  [ 2304/ 2380]\n",
      "loss: 2.407195  [ 2368/ 2380]\n",
      "loss: 2.434896  [  456/ 2380]\n",
      "Test Error: \n",
      " Avg loss: 2.383457 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.356341  [   64/ 2380]\n",
      "loss: 2.391489  [  128/ 2380]\n",
      "loss: 2.454857  [  192/ 2380]\n",
      "loss: 2.333303  [  256/ 2380]\n",
      "loss: 2.373519  [  320/ 2380]\n",
      "loss: 2.392380  [  384/ 2380]\n",
      "loss: 2.346740  [  448/ 2380]\n",
      "loss: 2.407299  [  512/ 2380]\n",
      "loss: 2.312537  [  576/ 2380]\n",
      "loss: 2.354629  [  640/ 2380]\n",
      "loss: 2.382441  [  704/ 2380]\n",
      "loss: 2.362674  [  768/ 2380]\n",
      "loss: 2.398416  [  832/ 2380]\n",
      "loss: 2.390818  [  896/ 2380]\n",
      "loss: 2.379673  [  960/ 2380]\n",
      "loss: 2.370549  [ 1024/ 2380]\n",
      "loss: 2.370540  [ 1088/ 2380]\n",
      "loss: 2.312211  [ 1152/ 2380]\n",
      "loss: 2.276065  [ 1216/ 2380]\n",
      "loss: 2.298509  [ 1280/ 2380]\n",
      "loss: 2.265050  [ 1344/ 2380]\n",
      "loss: 2.362427  [ 1408/ 2380]\n",
      "loss: 2.280686  [ 1472/ 2380]\n",
      "loss: 2.252683  [ 1536/ 2380]\n",
      "loss: 2.252084  [ 1600/ 2380]\n",
      "loss: 2.343404  [ 1664/ 2380]\n",
      "loss: 2.296369  [ 1728/ 2380]\n",
      "loss: 2.273495  [ 1792/ 2380]\n",
      "loss: 2.361926  [ 1856/ 2380]\n",
      "loss: 2.245027  [ 1920/ 2380]\n",
      "loss: 2.274404  [ 1984/ 2380]\n",
      "loss: 2.259286  [ 2048/ 2380]\n",
      "loss: 2.248141  [ 2112/ 2380]\n",
      "loss: 2.292576  [ 2176/ 2380]\n",
      "loss: 2.288607  [ 2240/ 2380]\n",
      "loss: 2.300782  [ 2304/ 2380]\n",
      "loss: 2.312491  [ 2368/ 2380]\n",
      "loss: 2.340053  [  456/ 2380]\n",
      "Test Error: \n",
      " Avg loss: 2.287743 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.258334  [   64/ 2380]\n",
      "loss: 2.296851  [  128/ 2380]\n",
      "loss: 2.363521  [  192/ 2380]\n",
      "loss: 2.234274  [  256/ 2380]\n",
      "loss: 2.279345  [  320/ 2380]\n",
      "loss: 2.303286  [  384/ 2380]\n",
      "loss: 2.253340  [  448/ 2380]\n",
      "loss: 2.315359  [  512/ 2380]\n",
      "loss: 2.215809  [  576/ 2380]\n",
      "loss: 2.264109  [  640/ 2380]\n",
      "loss: 2.290848  [  704/ 2380]\n",
      "loss: 2.273267  [  768/ 2380]\n",
      "loss: 2.311691  [  832/ 2380]\n",
      "loss: 2.302109  [  896/ 2380]\n",
      "loss: 2.290085  [  960/ 2380]\n",
      "loss: 2.281080  [ 1024/ 2380]\n",
      "loss: 2.284702  [ 1088/ 2380]\n",
      "loss: 2.221263  [ 1152/ 2380]\n",
      "loss: 2.189009  [ 1216/ 2380]\n",
      "loss: 2.210144  [ 1280/ 2380]\n",
      "loss: 2.174253  [ 1344/ 2380]\n",
      "loss: 2.278957  [ 1408/ 2380]\n",
      "loss: 2.192348  [ 1472/ 2380]\n",
      "loss: 2.166480  [ 1536/ 2380]\n",
      "loss: 2.164989  [ 1600/ 2380]\n",
      "loss: 2.259325  [ 1664/ 2380]\n",
      "loss: 2.208066  [ 1728/ 2380]\n",
      "loss: 2.188080  [ 1792/ 2380]\n",
      "loss: 2.276723  [ 1856/ 2380]\n",
      "loss: 2.161872  [ 1920/ 2380]\n",
      "loss: 2.187312  [ 1984/ 2380]\n",
      "loss: 2.177673  [ 2048/ 2380]\n",
      "loss: 2.160579  [ 2112/ 2380]\n",
      "loss: 2.212778  [ 2176/ 2380]\n",
      "loss: 2.206158  [ 2240/ 2380]\n",
      "loss: 2.218446  [ 2304/ 2380]\n",
      "loss: 2.232795  [ 2368/ 2380]\n",
      "loss: 2.260644  [  456/ 2380]\n",
      "Test Error: \n",
      " Avg loss: 2.207299 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.175673  [   64/ 2380]\n",
      "loss: 2.217058  [  128/ 2380]\n",
      "loss: 2.286205  [  192/ 2380]\n",
      "loss: 2.150815  [  256/ 2380]\n",
      "loss: 2.200610  [  320/ 2380]\n",
      "loss: 2.228248  [  384/ 2380]\n",
      "loss: 2.174665  [  448/ 2380]\n",
      "loss: 2.237808  [  512/ 2380]\n",
      "loss: 2.134209  [  576/ 2380]\n",
      "loss: 2.187650  [  640/ 2380]\n",
      "loss: 2.213311  [  704/ 2380]\n",
      "loss: 2.197339  [  768/ 2380]\n",
      "loss: 2.237972  [  832/ 2380]\n",
      "loss: 2.226327  [  896/ 2380]\n",
      "loss: 2.213644  [  960/ 2380]\n",
      "loss: 2.204786  [ 1024/ 2380]\n",
      "loss: 2.211645  [ 1088/ 2380]\n",
      "loss: 2.144023  [ 1152/ 2380]\n",
      "loss: 2.115894  [ 1216/ 2380]\n",
      "loss: 2.135176  [ 1280/ 2380]\n",
      "loss: 2.097338  [ 1344/ 2380]\n",
      "loss: 2.207985  [ 1408/ 2380]\n",
      "loss: 2.116959  [ 1472/ 2380]\n",
      "loss: 2.093548  [ 1536/ 2380]\n",
      "loss: 2.090710  [ 1600/ 2380]\n",
      "loss: 2.187859  [ 1664/ 2380]\n",
      "loss: 2.132955  [ 1728/ 2380]\n",
      "loss: 2.115656  [ 1792/ 2380]\n",
      "loss: 2.203018  [ 1856/ 2380]\n",
      "loss: 2.091471  [ 1920/ 2380]\n",
      "loss: 2.113189  [ 1984/ 2380]\n",
      "loss: 2.108299  [ 2048/ 2380]\n",
      "loss: 2.085927  [ 2112/ 2380]\n",
      "loss: 2.144855  [ 2176/ 2380]\n",
      "loss: 2.135704  [ 2240/ 2380]\n",
      "loss: 2.148166  [ 2304/ 2380]\n",
      "loss: 2.164317  [ 2368/ 2380]\n",
      "loss: 2.192966  [  456/ 2380]\n",
      "Test Error: \n",
      " Avg loss: 2.138679 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_loss</td><td>█▇▆▅▄▃▂▂▁▁</td></tr><tr><td>train_loss</td><td>██▇▇▇▆▆▆▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_loss</td><td>2.13868</td></tr><tr><td>train_loss</td><td>2.19297</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline</strong> at: <a href='https://wandb.ai/svenbbs/CS_challenge/runs/gfdw846c' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge/runs/gfdw846c</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240313_153128-gfdw846c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"CS_challenge\", name=\"Baseline\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": {lr_rate},\n",
    "        \"epochs\": {epochs},\n",
    "        \"Momentum\": {momentum},\n",
    "        \"Batch_size\": {batch_size},\n",
    "        \"model version\": 0.0,\n",
    "        \"subset size [%]\": 100,\n",
    "        \"resize\": {subsize},\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    run = train(train_dataloader, model, loss_fn, optimizer, run)\n",
    "    test_loss = test(val_dataloader, model, loss_fn)\n",
    "    run.log({\"Test_loss\": test_loss})\n",
    "print(\"Done!\")\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5lsm0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
