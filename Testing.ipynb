{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Resize, Compose, v2\n",
    "from torchvision.datasets import Cityscapes\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msvenbbs\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "subsize = (128, 256)\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:27: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:27: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\Sven\\AppData\\Local\\Temp\\ipykernel_11740\\3308899275.py:27: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  normal_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=normal_transform, target_transform=normal_target_transform)\n",
      "C:\\Users\\Sven\\AppData\\Local\\Temp\\ipykernel_11740\\3308899275.py:28: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  augmented_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=augmented_transform, target_transform=augmented_target_transform)\n",
      "c:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "augmented_transform = transforms.Compose([\n",
    "    v2.Resize(subsize),\n",
    "    v2.RandomHorizontalFlip(1),  # Random horizontal flip\n",
    "    v2.RandomRotation(10),  # Random rotation up to 10 degrees\n",
    "    v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jitter\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "augmented_target_transform = transforms.Compose([\n",
    "    v2.Resize(subsize),\n",
    "    v2.RandomHorizontalFlip(1),  # Same transformation as input images\n",
    "    v2.RandomRotation(10),  # Same transformation as input images\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "normal_transform = transforms.Compose([\n",
    "    v2.Resize(subsize),\n",
    "    v2.ToTensor()\n",
    "])\n",
    "normal_target_transform = transforms.Compose([\n",
    "    v2.Resize(subsize),\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "#Desktop\n",
    "normal_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=normal_transform, target_transform=normal_target_transform)\n",
    "augmented_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=augmented_transform, target_transform=augmented_target_transform)\n",
    "dataset = torch.utils.data.ConcatDataset([normal_dataset, augmented_dataset])\n",
    "\n",
    "\n",
    "#Laptop\n",
    "#dataset = Cityscapes(root=\"C:/Users/20182573/Documents/CityScapes\", split='train', mode='fine', target_type='semantic', transform=transform, target_transform=target_transform)\n",
    "\n",
    "#subset_small, subset_big = random_split(dataset, [0.2,0.8])\n",
    "train_dataset, val_dataset = random_split(dataset, [0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4760"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model import Model\n",
    "\n",
    "model = Model().cuda()\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, run):\n",
    "    \"\"\"\n",
    "    Train a model for 1 epoch.\n",
    "\n",
    "    Params:\n",
    "    - dataloader:   dataset to train on.\n",
    "    - model:        the model object to be trained.\n",
    "    - loss_fn:      the loss function.\n",
    "    - optimizer:    the desired optimization.\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() #Set the model to train mode\n",
    "    for batch, (IMG,SEGM) in enumerate(dataloader):\n",
    "        IMG = IMG.to('cuda')\n",
    "        SEGM  = (SEGM*255).long().squeeze()     #*255 because the id are normalized between 0-1\n",
    "        SEGM = utils.map_id_to_train_id(SEGM).to('cuda')\n",
    "        \n",
    "        #predict\n",
    "        pred = model(IMG)\n",
    "        #Loss\n",
    "        loss = loss_fn(pred, SEGM)\n",
    "        \n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #print loss during training\n",
    "        loss, current = loss.item(), (batch + 1) * len(IMG)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        run.log({\"train_loss\": loss})\n",
    "    \n",
    "    return run\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Test a model.\n",
    "\n",
    "    Params:\n",
    "    - dataloader:   dataset to test on.\n",
    "    - model:        the model object to be tested.\n",
    "    - loss_fn:      the loss function.\n",
    "    \"\"\"\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() #model in eval mode\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (IMG,SEGM) in enumerate(dataloader):\n",
    "            IMG = IMG.to('cuda')\n",
    "            SEGM  = (SEGM*255).long().squeeze()     #*255 because the id are normalized between 0-1\n",
    "            SEGM = utils.map_id_to_train_id(SEGM).to('cuda')\n",
    "\n",
    "            pred = model(IMG)\n",
    "            test_loss += loss_fn(pred, SEGM).item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Sven\\Documents\\GitHub\\CS_challenge\\wandb\\run-20240320_115929-6n2vtvi4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/svenbbs/CS_challenge/runs/6n2vtvi4' target=\"_blank\">1.2</a></strong> to <a href='https://wandb.ai/svenbbs/CS_challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/svenbbs/CS_challenge' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/svenbbs/CS_challenge/runs/6n2vtvi4' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge/runs/6n2vtvi4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.508835  [   64/ 4760]\n",
      "loss: 3.502861  [  128/ 4760]\n",
      "loss: 3.481373  [  192/ 4760]\n",
      "loss: 3.464029  [  256/ 4760]\n",
      "loss: 3.432542  [  320/ 4760]\n",
      "loss: 3.381169  [  384/ 4760]\n",
      "loss: 3.361730  [  448/ 4760]\n",
      "loss: 3.311038  [  512/ 4760]\n",
      "loss: 3.277574  [  576/ 4760]\n",
      "loss: 3.214880  [  640/ 4760]\n",
      "loss: 3.213984  [  704/ 4760]\n",
      "loss: 3.159724  [  768/ 4760]\n",
      "loss: 3.138999  [  832/ 4760]\n",
      "loss: 3.126931  [  896/ 4760]\n",
      "loss: 3.088607  [  960/ 4760]\n",
      "loss: 3.024116  [ 1024/ 4760]\n",
      "loss: 3.001950  [ 1088/ 4760]\n",
      "loss: 2.954829  [ 1152/ 4760]\n",
      "loss: 2.960425  [ 1216/ 4760]\n",
      "loss: 2.922375  [ 1280/ 4760]\n",
      "loss: 2.918529  [ 1344/ 4760]\n",
      "loss: 2.866815  [ 1408/ 4760]\n",
      "loss: 2.823286  [ 1472/ 4760]\n",
      "loss: 2.837028  [ 1536/ 4760]\n",
      "loss: 2.752928  [ 1600/ 4760]\n",
      "loss: 2.808018  [ 1664/ 4760]\n",
      "loss: 2.714179  [ 1728/ 4760]\n",
      "loss: 2.728692  [ 1792/ 4760]\n",
      "loss: 2.682433  [ 1856/ 4760]\n",
      "loss: 2.651441  [ 1920/ 4760]\n",
      "loss: 2.654855  [ 1984/ 4760]\n",
      "loss: 2.631432  [ 2048/ 4760]\n",
      "loss: 2.567651  [ 2112/ 4760]\n",
      "loss: 2.570964  [ 2176/ 4760]\n",
      "loss: 2.586119  [ 2240/ 4760]\n",
      "loss: 2.564111  [ 2304/ 4760]\n",
      "loss: 2.499451  [ 2368/ 4760]\n",
      "loss: 2.509515  [ 2432/ 4760]\n",
      "loss: 2.469456  [ 2496/ 4760]\n",
      "loss: 2.474864  [ 2560/ 4760]\n",
      "loss: 2.406218  [ 2624/ 4760]\n",
      "loss: 2.439651  [ 2688/ 4760]\n",
      "loss: 2.394341  [ 2752/ 4760]\n",
      "loss: 2.466120  [ 2816/ 4760]\n",
      "loss: 2.393629  [ 2880/ 4760]\n",
      "loss: 2.371887  [ 2944/ 4760]\n",
      "loss: 2.350054  [ 3008/ 4760]\n",
      "loss: 2.363763  [ 3072/ 4760]\n",
      "loss: 2.404603  [ 3136/ 4760]\n",
      "loss: 2.354115  [ 3200/ 4760]\n",
      "loss: 2.367443  [ 3264/ 4760]\n",
      "loss: 2.302972  [ 3328/ 4760]\n",
      "loss: 2.274581  [ 3392/ 4760]\n",
      "loss: 2.269528  [ 3456/ 4760]\n",
      "loss: 2.293495  [ 3520/ 4760]\n",
      "loss: 2.227654  [ 3584/ 4760]\n",
      "loss: 2.188504  [ 3648/ 4760]\n",
      "loss: 2.119898  [ 3712/ 4760]\n",
      "loss: 2.234956  [ 3776/ 4760]\n",
      "loss: 2.212316  [ 3840/ 4760]\n",
      "loss: 2.145305  [ 3904/ 4760]\n",
      "loss: 2.121361  [ 3968/ 4760]\n",
      "loss: 2.183727  [ 4032/ 4760]\n",
      "loss: 2.203042  [ 4096/ 4760]\n",
      "loss: 2.179424  [ 4160/ 4760]\n",
      "loss: 2.054567  [ 4224/ 4760]\n",
      "loss: 2.069288  [ 4288/ 4760]\n",
      "loss: 2.082359  [ 4352/ 4760]\n",
      "loss: 2.043894  [ 4416/ 4760]\n",
      "loss: 2.072230  [ 4480/ 4760]\n",
      "loss: 2.078912  [ 4544/ 4760]\n",
      "loss: 2.029236  [ 4608/ 4760]\n",
      "loss: 2.061643  [ 4672/ 4760]\n",
      "loss: 2.096435  [ 4736/ 4760]\n",
      "loss: 2.031013  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 2.088356 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.028701  [   64/ 4760]\n",
      "loss: 1.975858  [  128/ 4760]\n",
      "loss: 1.916479  [  192/ 4760]\n",
      "loss: 1.974422  [  256/ 4760]\n",
      "loss: 2.003814  [  320/ 4760]\n",
      "loss: 1.930730  [  384/ 4760]\n",
      "loss: 1.970839  [  448/ 4760]\n",
      "loss: 2.046680  [  512/ 4760]\n",
      "loss: 1.922361  [  576/ 4760]\n",
      "loss: 1.898412  [  640/ 4760]\n",
      "loss: 1.914075  [  704/ 4760]\n",
      "loss: 1.906180  [  768/ 4760]\n",
      "loss: 1.978007  [  832/ 4760]\n",
      "loss: 1.934040  [  896/ 4760]\n",
      "loss: 1.879497  [  960/ 4760]\n",
      "loss: 1.965794  [ 1024/ 4760]\n",
      "loss: 1.854056  [ 1088/ 4760]\n",
      "loss: 1.907209  [ 1152/ 4760]\n",
      "loss: 1.891681  [ 1216/ 4760]\n",
      "loss: 1.950068  [ 1280/ 4760]\n",
      "loss: 1.816390  [ 1344/ 4760]\n",
      "loss: 1.857328  [ 1408/ 4760]\n",
      "loss: 1.873749  [ 1472/ 4760]\n",
      "loss: 1.818927  [ 1536/ 4760]\n",
      "loss: 1.811173  [ 1600/ 4760]\n",
      "loss: 1.779131  [ 1664/ 4760]\n",
      "loss: 1.840369  [ 1728/ 4760]\n",
      "loss: 1.873261  [ 1792/ 4760]\n",
      "loss: 1.849046  [ 1856/ 4760]\n",
      "loss: 1.798630  [ 1920/ 4760]\n",
      "loss: 1.817970  [ 1984/ 4760]\n",
      "loss: 1.786193  [ 2048/ 4760]\n",
      "loss: 1.870695  [ 2112/ 4760]\n",
      "loss: 1.742252  [ 2176/ 4760]\n",
      "loss: 1.758155  [ 2240/ 4760]\n",
      "loss: 1.811420  [ 2304/ 4760]\n",
      "loss: 1.796812  [ 2368/ 4760]\n",
      "loss: 1.776017  [ 2432/ 4760]\n",
      "loss: 1.791934  [ 2496/ 4760]\n",
      "loss: 1.759393  [ 2560/ 4760]\n",
      "loss: 1.717085  [ 2624/ 4760]\n",
      "loss: 1.733984  [ 2688/ 4760]\n",
      "loss: 1.824890  [ 2752/ 4760]\n",
      "loss: 1.700378  [ 2816/ 4760]\n",
      "loss: 1.778720  [ 2880/ 4760]\n",
      "loss: 1.724141  [ 2944/ 4760]\n",
      "loss: 1.826007  [ 3008/ 4760]\n",
      "loss: 1.656837  [ 3072/ 4760]\n",
      "loss: 1.710907  [ 3136/ 4760]\n",
      "loss: 1.673205  [ 3200/ 4760]\n",
      "loss: 1.658107  [ 3264/ 4760]\n",
      "loss: 1.730867  [ 3328/ 4760]\n",
      "loss: 1.644844  [ 3392/ 4760]\n",
      "loss: 1.681823  [ 3456/ 4760]\n",
      "loss: 1.644681  [ 3520/ 4760]\n",
      "loss: 1.651362  [ 3584/ 4760]\n",
      "loss: 1.640812  [ 3648/ 4760]\n",
      "loss: 1.642580  [ 3712/ 4760]\n",
      "loss: 1.641335  [ 3776/ 4760]\n",
      "loss: 1.680579  [ 3840/ 4760]\n",
      "loss: 1.609684  [ 3904/ 4760]\n",
      "loss: 1.620544  [ 3968/ 4760]\n",
      "loss: 1.684343  [ 4032/ 4760]\n",
      "loss: 1.607896  [ 4096/ 4760]\n",
      "loss: 1.597038  [ 4160/ 4760]\n",
      "loss: 1.541455  [ 4224/ 4760]\n",
      "loss: 1.566126  [ 4288/ 4760]\n",
      "loss: 1.614917  [ 4352/ 4760]\n",
      "loss: 1.594728  [ 4416/ 4760]\n",
      "loss: 1.486810  [ 4480/ 4760]\n",
      "loss: 1.583524  [ 4544/ 4760]\n",
      "loss: 1.622294  [ 4608/ 4760]\n",
      "loss: 1.658734  [ 4672/ 4760]\n",
      "loss: 1.538467  [ 4736/ 4760]\n",
      "loss: 1.494627  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 1.616055 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.629741  [   64/ 4760]\n",
      "loss: 1.489251  [  128/ 4760]\n",
      "loss: 1.576323  [  192/ 4760]\n",
      "loss: 1.527712  [  256/ 4760]\n",
      "loss: 1.583076  [  320/ 4760]\n",
      "loss: 1.635458  [  384/ 4760]\n",
      "loss: 1.587531  [  448/ 4760]\n",
      "loss: 1.651556  [  512/ 4760]\n",
      "loss: 1.547780  [  576/ 4760]\n",
      "loss: 1.602724  [  640/ 4760]\n",
      "loss: 1.522294  [  704/ 4760]\n",
      "loss: 1.578726  [  768/ 4760]\n",
      "loss: 1.592139  [  832/ 4760]\n",
      "loss: 1.504886  [  896/ 4760]\n",
      "loss: 1.560622  [  960/ 4760]\n",
      "loss: 1.610373  [ 1024/ 4760]\n",
      "loss: 1.503728  [ 1088/ 4760]\n",
      "loss: 1.541309  [ 1152/ 4760]\n",
      "loss: 1.578943  [ 1216/ 4760]\n",
      "loss: 1.478923  [ 1280/ 4760]\n",
      "loss: 1.545835  [ 1344/ 4760]\n",
      "loss: 1.540365  [ 1408/ 4760]\n",
      "loss: 1.538263  [ 1472/ 4760]\n",
      "loss: 1.525004  [ 1536/ 4760]\n",
      "loss: 1.531264  [ 1600/ 4760]\n",
      "loss: 1.534500  [ 1664/ 4760]\n",
      "loss: 1.433528  [ 1728/ 4760]\n",
      "loss: 1.529950  [ 1792/ 4760]\n",
      "loss: 1.491527  [ 1856/ 4760]\n",
      "loss: 1.464222  [ 1920/ 4760]\n",
      "loss: 1.544398  [ 1984/ 4760]\n",
      "loss: 1.421084  [ 2048/ 4760]\n",
      "loss: 1.476985  [ 2112/ 4760]\n",
      "loss: 1.493307  [ 2176/ 4760]\n",
      "loss: 1.539598  [ 2240/ 4760]\n",
      "loss: 1.501231  [ 2304/ 4760]\n",
      "loss: 1.449440  [ 2368/ 4760]\n",
      "loss: 1.479722  [ 2432/ 4760]\n",
      "loss: 1.462933  [ 2496/ 4760]\n",
      "loss: 1.441223  [ 2560/ 4760]\n",
      "loss: 1.432055  [ 2624/ 4760]\n",
      "loss: 1.419529  [ 2688/ 4760]\n",
      "loss: 1.426641  [ 2752/ 4760]\n",
      "loss: 1.475560  [ 2816/ 4760]\n",
      "loss: 1.510986  [ 2880/ 4760]\n",
      "loss: 1.435898  [ 2944/ 4760]\n",
      "loss: 1.519308  [ 3008/ 4760]\n",
      "loss: 1.511883  [ 3072/ 4760]\n",
      "loss: 1.455941  [ 3136/ 4760]\n",
      "loss: 1.392060  [ 3200/ 4760]\n",
      "loss: 1.433032  [ 3264/ 4760]\n",
      "loss: 1.462273  [ 3328/ 4760]\n",
      "loss: 1.418418  [ 3392/ 4760]\n",
      "loss: 1.428249  [ 3456/ 4760]\n",
      "loss: 1.502050  [ 3520/ 4760]\n",
      "loss: 1.432888  [ 3584/ 4760]\n",
      "loss: 1.433643  [ 3648/ 4760]\n",
      "loss: 1.434030  [ 3712/ 4760]\n",
      "loss: 1.410981  [ 3776/ 4760]\n",
      "loss: 1.396951  [ 3840/ 4760]\n",
      "loss: 1.361349  [ 3904/ 4760]\n",
      "loss: 1.414817  [ 3968/ 4760]\n",
      "loss: 1.456315  [ 4032/ 4760]\n",
      "loss: 1.349198  [ 4096/ 4760]\n",
      "loss: 1.456887  [ 4160/ 4760]\n",
      "loss: 1.391527  [ 4224/ 4760]\n",
      "loss: 1.409550  [ 4288/ 4760]\n",
      "loss: 1.426867  [ 4352/ 4760]\n",
      "loss: 1.413256  [ 4416/ 4760]\n",
      "loss: 1.301436  [ 4480/ 4760]\n",
      "loss: 1.379006  [ 4544/ 4760]\n",
      "loss: 1.346878  [ 4608/ 4760]\n",
      "loss: 1.326738  [ 4672/ 4760]\n",
      "loss: 1.400220  [ 4736/ 4760]\n",
      "loss: 1.426585  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 1.402775 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.447402  [   64/ 4760]\n",
      "loss: 1.356517  [  128/ 4760]\n",
      "loss: 1.378855  [  192/ 4760]\n",
      "loss: 1.386686  [  256/ 4760]\n",
      "loss: 1.321321  [  320/ 4760]\n",
      "loss: 1.350566  [  384/ 4760]\n",
      "loss: 1.404146  [  448/ 4760]\n",
      "loss: 1.341547  [  512/ 4760]\n",
      "loss: 1.442026  [  576/ 4760]\n",
      "loss: 1.401754  [  640/ 4760]\n",
      "loss: 1.333665  [  704/ 4760]\n",
      "loss: 1.319546  [  768/ 4760]\n",
      "loss: 1.341229  [  832/ 4760]\n",
      "loss: 1.359255  [  896/ 4760]\n",
      "loss: 1.385953  [  960/ 4760]\n",
      "loss: 1.272354  [ 1024/ 4760]\n",
      "loss: 1.329129  [ 1088/ 4760]\n",
      "loss: 1.385618  [ 1152/ 4760]\n",
      "loss: 1.255637  [ 1216/ 4760]\n",
      "loss: 1.295876  [ 1280/ 4760]\n",
      "loss: 1.366847  [ 1344/ 4760]\n",
      "loss: 1.282324  [ 1408/ 4760]\n",
      "loss: 1.300307  [ 1472/ 4760]\n",
      "loss: 1.357278  [ 1536/ 4760]\n",
      "loss: 1.360553  [ 1600/ 4760]\n",
      "loss: 1.311968  [ 1664/ 4760]\n",
      "loss: 1.363767  [ 1728/ 4760]\n",
      "loss: 1.400123  [ 1792/ 4760]\n",
      "loss: 1.364812  [ 1856/ 4760]\n",
      "loss: 1.314487  [ 1920/ 4760]\n",
      "loss: 1.321018  [ 1984/ 4760]\n",
      "loss: 1.247965  [ 2048/ 4760]\n",
      "loss: 1.250656  [ 2112/ 4760]\n",
      "loss: 1.341265  [ 2176/ 4760]\n",
      "loss: 1.352243  [ 2240/ 4760]\n",
      "loss: 1.339401  [ 2304/ 4760]\n",
      "loss: 1.331491  [ 2368/ 4760]\n",
      "loss: 1.330072  [ 2432/ 4760]\n",
      "loss: 1.208811  [ 2496/ 4760]\n",
      "loss: 1.410482  [ 2560/ 4760]\n",
      "loss: 1.406338  [ 2624/ 4760]\n",
      "loss: 1.276040  [ 2688/ 4760]\n",
      "loss: 1.287119  [ 2752/ 4760]\n",
      "loss: 1.270583  [ 2816/ 4760]\n",
      "loss: 1.272511  [ 2880/ 4760]\n",
      "loss: 1.271828  [ 2944/ 4760]\n",
      "loss: 1.298256  [ 3008/ 4760]\n",
      "loss: 1.354960  [ 3072/ 4760]\n",
      "loss: 1.306394  [ 3136/ 4760]\n",
      "loss: 1.269742  [ 3200/ 4760]\n",
      "loss: 1.314457  [ 3264/ 4760]\n",
      "loss: 1.298054  [ 3328/ 4760]\n",
      "loss: 1.303875  [ 3392/ 4760]\n",
      "loss: 1.245536  [ 3456/ 4760]\n",
      "loss: 1.283834  [ 3520/ 4760]\n",
      "loss: 1.268212  [ 3584/ 4760]\n",
      "loss: 1.267053  [ 3648/ 4760]\n",
      "loss: 1.226092  [ 3712/ 4760]\n",
      "loss: 1.246500  [ 3776/ 4760]\n",
      "loss: 1.285814  [ 3840/ 4760]\n",
      "loss: 1.299203  [ 3904/ 4760]\n",
      "loss: 1.230632  [ 3968/ 4760]\n",
      "loss: 1.251609  [ 4032/ 4760]\n",
      "loss: 1.271481  [ 4096/ 4760]\n",
      "loss: 1.238834  [ 4160/ 4760]\n",
      "loss: 1.240969  [ 4224/ 4760]\n",
      "loss: 1.225327  [ 4288/ 4760]\n",
      "loss: 1.229463  [ 4352/ 4760]\n",
      "loss: 1.214559  [ 4416/ 4760]\n",
      "loss: 1.282725  [ 4480/ 4760]\n",
      "loss: 1.164491  [ 4544/ 4760]\n",
      "loss: 1.250023  [ 4608/ 4760]\n",
      "loss: 1.170091  [ 4672/ 4760]\n",
      "loss: 1.243689  [ 4736/ 4760]\n",
      "loss: 1.118500  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 1.253694 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.187350  [   64/ 4760]\n",
      "loss: 1.246815  [  128/ 4760]\n",
      "loss: 1.218540  [  192/ 4760]\n",
      "loss: 1.262581  [  256/ 4760]\n",
      "loss: 1.233761  [  320/ 4760]\n",
      "loss: 1.225625  [  384/ 4760]\n",
      "loss: 1.180240  [  448/ 4760]\n",
      "loss: 1.259526  [  512/ 4760]\n",
      "loss: 1.215639  [  576/ 4760]\n",
      "loss: 1.160376  [  640/ 4760]\n",
      "loss: 1.293570  [  704/ 4760]\n",
      "loss: 1.297297  [  768/ 4760]\n",
      "loss: 1.243404  [  832/ 4760]\n",
      "loss: 1.201010  [  896/ 4760]\n",
      "loss: 1.220561  [  960/ 4760]\n",
      "loss: 1.317429  [ 1024/ 4760]\n",
      "loss: 1.191860  [ 1088/ 4760]\n",
      "loss: 1.131478  [ 1152/ 4760]\n",
      "loss: 1.234142  [ 1216/ 4760]\n",
      "loss: 1.185745  [ 1280/ 4760]\n",
      "loss: 1.145116  [ 1344/ 4760]\n",
      "loss: 1.160722  [ 1408/ 4760]\n",
      "loss: 1.219054  [ 1472/ 4760]\n",
      "loss: 1.322374  [ 1536/ 4760]\n",
      "loss: 1.161650  [ 1600/ 4760]\n",
      "loss: 1.158134  [ 1664/ 4760]\n",
      "loss: 1.198889  [ 1728/ 4760]\n",
      "loss: 1.266750  [ 1792/ 4760]\n",
      "loss: 1.209240  [ 1856/ 4760]\n",
      "loss: 1.103969  [ 1920/ 4760]\n",
      "loss: 1.191223  [ 1984/ 4760]\n",
      "loss: 1.254505  [ 2048/ 4760]\n",
      "loss: 1.206333  [ 2112/ 4760]\n",
      "loss: 1.206840  [ 2176/ 4760]\n",
      "loss: 1.217942  [ 2240/ 4760]\n",
      "loss: 1.220478  [ 2304/ 4760]\n",
      "loss: 1.186446  [ 2368/ 4760]\n",
      "loss: 1.197459  [ 2432/ 4760]\n",
      "loss: 1.143718  [ 2496/ 4760]\n",
      "loss: 1.152855  [ 2560/ 4760]\n",
      "loss: 1.196464  [ 2624/ 4760]\n",
      "loss: 1.166066  [ 2688/ 4760]\n",
      "loss: 1.233777  [ 2752/ 4760]\n",
      "loss: 1.159163  [ 2816/ 4760]\n",
      "loss: 1.210607  [ 2880/ 4760]\n",
      "loss: 1.157728  [ 2944/ 4760]\n",
      "loss: 1.277866  [ 3008/ 4760]\n",
      "loss: 1.211516  [ 3072/ 4760]\n",
      "loss: 1.191922  [ 3136/ 4760]\n",
      "loss: 1.126107  [ 3200/ 4760]\n",
      "loss: 1.160063  [ 3264/ 4760]\n",
      "loss: 1.135859  [ 3328/ 4760]\n",
      "loss: 1.207338  [ 3392/ 4760]\n",
      "loss: 1.191096  [ 3456/ 4760]\n",
      "loss: 1.166606  [ 3520/ 4760]\n",
      "loss: 1.196883  [ 3584/ 4760]\n",
      "loss: 1.147129  [ 3648/ 4760]\n",
      "loss: 1.174565  [ 3712/ 4760]\n",
      "loss: 1.227001  [ 3776/ 4760]\n",
      "loss: 1.177457  [ 3840/ 4760]\n",
      "loss: 1.145814  [ 3904/ 4760]\n",
      "loss: 1.123582  [ 3968/ 4760]\n",
      "loss: 1.137437  [ 4032/ 4760]\n",
      "loss: 1.172300  [ 4096/ 4760]\n",
      "loss: 1.156114  [ 4160/ 4760]\n",
      "loss: 1.225912  [ 4224/ 4760]\n",
      "loss: 1.138750  [ 4288/ 4760]\n",
      "loss: 1.139494  [ 4352/ 4760]\n",
      "loss: 1.126939  [ 4416/ 4760]\n",
      "loss: 1.094332  [ 4480/ 4760]\n",
      "loss: 1.130752  [ 4544/ 4760]\n",
      "loss: 1.153504  [ 4608/ 4760]\n",
      "loss: 1.157255  [ 4672/ 4760]\n",
      "loss: 1.129551  [ 4736/ 4760]\n",
      "loss: 1.141376  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 1.187439 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.172216  [   64/ 4760]\n",
      "loss: 1.172312  [  128/ 4760]\n",
      "loss: 1.062878  [  192/ 4760]\n",
      "loss: 1.145708  [  256/ 4760]\n",
      "loss: 1.076871  [  320/ 4760]\n",
      "loss: 1.179507  [  384/ 4760]\n",
      "loss: 1.106955  [  448/ 4760]\n",
      "loss: 1.088792  [  512/ 4760]\n",
      "loss: 1.132508  [  576/ 4760]\n",
      "loss: 1.058420  [  640/ 4760]\n",
      "loss: 1.174660  [  704/ 4760]\n",
      "loss: 1.124148  [  768/ 4760]\n",
      "loss: 1.119241  [  832/ 4760]\n",
      "loss: 1.234980  [  896/ 4760]\n",
      "loss: 1.114106  [  960/ 4760]\n",
      "loss: 1.129124  [ 1024/ 4760]\n",
      "loss: 1.184785  [ 1088/ 4760]\n",
      "loss: 1.184070  [ 1152/ 4760]\n",
      "loss: 1.129185  [ 1216/ 4760]\n",
      "loss: 1.161142  [ 1280/ 4760]\n",
      "loss: 1.152621  [ 1344/ 4760]\n",
      "loss: 1.153514  [ 1408/ 4760]\n",
      "loss: 1.139100  [ 1472/ 4760]\n",
      "loss: 1.136178  [ 1536/ 4760]\n",
      "loss: 1.181010  [ 1600/ 4760]\n",
      "loss: 1.092883  [ 1664/ 4760]\n",
      "loss: 1.192389  [ 1728/ 4760]\n",
      "loss: 1.150492  [ 1792/ 4760]\n",
      "loss: 1.144567  [ 1856/ 4760]\n",
      "loss: 1.129321  [ 1920/ 4760]\n",
      "loss: 1.178664  [ 1984/ 4760]\n",
      "loss: 1.158196  [ 2048/ 4760]\n",
      "loss: 1.049659  [ 2112/ 4760]\n",
      "loss: 1.129172  [ 2176/ 4760]\n",
      "loss: 1.087176  [ 2240/ 4760]\n",
      "loss: 1.233279  [ 2304/ 4760]\n",
      "loss: 1.063743  [ 2368/ 4760]\n",
      "loss: 1.113706  [ 2432/ 4760]\n",
      "loss: 1.208021  [ 2496/ 4760]\n",
      "loss: 1.059822  [ 2560/ 4760]\n",
      "loss: 1.089055  [ 2624/ 4760]\n",
      "loss: 1.156344  [ 2688/ 4760]\n",
      "loss: 1.126352  [ 2752/ 4760]\n",
      "loss: 1.144041  [ 2816/ 4760]\n",
      "loss: 1.100951  [ 2880/ 4760]\n",
      "loss: 1.164198  [ 2944/ 4760]\n",
      "loss: 1.010977  [ 3008/ 4760]\n",
      "loss: 1.050801  [ 3072/ 4760]\n",
      "loss: 1.119879  [ 3136/ 4760]\n",
      "loss: 1.114001  [ 3200/ 4760]\n",
      "loss: 1.039199  [ 3264/ 4760]\n",
      "loss: 1.066077  [ 3328/ 4760]\n",
      "loss: 1.056340  [ 3392/ 4760]\n",
      "loss: 1.080850  [ 3456/ 4760]\n",
      "loss: 1.112357  [ 3520/ 4760]\n",
      "loss: 1.096857  [ 3584/ 4760]\n",
      "loss: 1.082624  [ 3648/ 4760]\n",
      "loss: 1.051487  [ 3712/ 4760]\n",
      "loss: 1.054638  [ 3776/ 4760]\n",
      "loss: 1.050809  [ 3840/ 4760]\n",
      "loss: 1.099258  [ 3904/ 4760]\n",
      "loss: 1.035423  [ 3968/ 4760]\n",
      "loss: 1.146668  [ 4032/ 4760]\n",
      "loss: 1.116211  [ 4096/ 4760]\n",
      "loss: 1.065991  [ 4160/ 4760]\n",
      "loss: 1.108357  [ 4224/ 4760]\n",
      "loss: 1.078985  [ 4288/ 4760]\n",
      "loss: 1.059827  [ 4352/ 4760]\n",
      "loss: 1.032587  [ 4416/ 4760]\n",
      "loss: 1.115632  [ 4480/ 4760]\n",
      "loss: 1.128136  [ 4544/ 4760]\n",
      "loss: 1.054552  [ 4608/ 4760]\n",
      "loss: 1.030161  [ 4672/ 4760]\n",
      "loss: 1.036213  [ 4736/ 4760]\n",
      "loss: 1.096908  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 1.084389 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.040029  [   64/ 4760]\n",
      "loss: 1.030154  [  128/ 4760]\n",
      "loss: 0.966396  [  192/ 4760]\n",
      "loss: 1.011379  [  256/ 4760]\n",
      "loss: 1.071416  [  320/ 4760]\n",
      "loss: 1.061077  [  384/ 4760]\n",
      "loss: 1.064203  [  448/ 4760]\n",
      "loss: 1.062009  [  512/ 4760]\n",
      "loss: 1.026649  [  576/ 4760]\n",
      "loss: 1.102089  [  640/ 4760]\n",
      "loss: 1.105420  [  704/ 4760]\n",
      "loss: 0.998187  [  768/ 4760]\n",
      "loss: 1.054449  [  832/ 4760]\n",
      "loss: 1.077292  [  896/ 4760]\n",
      "loss: 1.090415  [  960/ 4760]\n",
      "loss: 1.060903  [ 1024/ 4760]\n",
      "loss: 1.099664  [ 1088/ 4760]\n",
      "loss: 1.059104  [ 1152/ 4760]\n",
      "loss: 1.017558  [ 1216/ 4760]\n",
      "loss: 1.066815  [ 1280/ 4760]\n",
      "loss: 1.171508  [ 1344/ 4760]\n",
      "loss: 1.104318  [ 1408/ 4760]\n",
      "loss: 1.048151  [ 1472/ 4760]\n",
      "loss: 1.030763  [ 1536/ 4760]\n",
      "loss: 1.019305  [ 1600/ 4760]\n",
      "loss: 1.114092  [ 1664/ 4760]\n",
      "loss: 1.042842  [ 1728/ 4760]\n",
      "loss: 1.012195  [ 1792/ 4760]\n",
      "loss: 1.043476  [ 1856/ 4760]\n",
      "loss: 1.085678  [ 1920/ 4760]\n",
      "loss: 0.976429  [ 1984/ 4760]\n",
      "loss: 1.073590  [ 2048/ 4760]\n",
      "loss: 1.052968  [ 2112/ 4760]\n",
      "loss: 1.019124  [ 2176/ 4760]\n",
      "loss: 1.032019  [ 2240/ 4760]\n",
      "loss: 1.046889  [ 2304/ 4760]\n",
      "loss: 1.086282  [ 2368/ 4760]\n",
      "loss: 1.055528  [ 2432/ 4760]\n",
      "loss: 0.979464  [ 2496/ 4760]\n",
      "loss: 1.040839  [ 2560/ 4760]\n",
      "loss: 1.059144  [ 2624/ 4760]\n",
      "loss: 0.977300  [ 2688/ 4760]\n",
      "loss: 1.061563  [ 2752/ 4760]\n",
      "loss: 1.069760  [ 2816/ 4760]\n",
      "loss: 1.110391  [ 2880/ 4760]\n",
      "loss: 1.082446  [ 2944/ 4760]\n",
      "loss: 0.994117  [ 3008/ 4760]\n",
      "loss: 1.044590  [ 3072/ 4760]\n",
      "loss: 1.098676  [ 3136/ 4760]\n",
      "loss: 1.080474  [ 3200/ 4760]\n",
      "loss: 1.095813  [ 3264/ 4760]\n",
      "loss: 1.093992  [ 3328/ 4760]\n",
      "loss: 1.094961  [ 3392/ 4760]\n",
      "loss: 1.032214  [ 3456/ 4760]\n",
      "loss: 1.104931  [ 3520/ 4760]\n",
      "loss: 1.021875  [ 3584/ 4760]\n",
      "loss: 1.022453  [ 3648/ 4760]\n",
      "loss: 0.977917  [ 3712/ 4760]\n",
      "loss: 1.021373  [ 3776/ 4760]\n",
      "loss: 1.025028  [ 3840/ 4760]\n",
      "loss: 0.992166  [ 3904/ 4760]\n",
      "loss: 1.046729  [ 3968/ 4760]\n",
      "loss: 1.008704  [ 4032/ 4760]\n",
      "loss: 0.994870  [ 4096/ 4760]\n",
      "loss: 1.001104  [ 4160/ 4760]\n",
      "loss: 1.044715  [ 4224/ 4760]\n",
      "loss: 0.990782  [ 4288/ 4760]\n",
      "loss: 1.027938  [ 4352/ 4760]\n",
      "loss: 0.976386  [ 4416/ 4760]\n",
      "loss: 1.004003  [ 4480/ 4760]\n",
      "loss: 1.052548  [ 4544/ 4760]\n",
      "loss: 1.026644  [ 4608/ 4760]\n",
      "loss: 1.029409  [ 4672/ 4760]\n",
      "loss: 1.032469  [ 4736/ 4760]\n",
      "loss: 1.068705  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 1.038377 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.987936  [   64/ 4760]\n",
      "loss: 1.039664  [  128/ 4760]\n",
      "loss: 0.989541  [  192/ 4760]\n",
      "loss: 1.018957  [  256/ 4760]\n",
      "loss: 1.005098  [  320/ 4760]\n",
      "loss: 0.972272  [  384/ 4760]\n",
      "loss: 1.064952  [  448/ 4760]\n",
      "loss: 1.048585  [  512/ 4760]\n",
      "loss: 1.046880  [  576/ 4760]\n",
      "loss: 1.002436  [  640/ 4760]\n",
      "loss: 1.025915  [  704/ 4760]\n",
      "loss: 1.015551  [  768/ 4760]\n",
      "loss: 1.036299  [  832/ 4760]\n",
      "loss: 0.996499  [  896/ 4760]\n",
      "loss: 0.947028  [  960/ 4760]\n",
      "loss: 1.000221  [ 1024/ 4760]\n",
      "loss: 1.023706  [ 1088/ 4760]\n",
      "loss: 1.090091  [ 1152/ 4760]\n",
      "loss: 1.018981  [ 1216/ 4760]\n",
      "loss: 1.009264  [ 1280/ 4760]\n",
      "loss: 0.940443  [ 1344/ 4760]\n",
      "loss: 0.954332  [ 1408/ 4760]\n",
      "loss: 1.037941  [ 1472/ 4760]\n",
      "loss: 1.022649  [ 1536/ 4760]\n",
      "loss: 0.967914  [ 1600/ 4760]\n",
      "loss: 1.061032  [ 1664/ 4760]\n",
      "loss: 0.980836  [ 1728/ 4760]\n",
      "loss: 0.969496  [ 1792/ 4760]\n",
      "loss: 1.079463  [ 1856/ 4760]\n",
      "loss: 1.017316  [ 1920/ 4760]\n",
      "loss: 1.023128  [ 1984/ 4760]\n",
      "loss: 0.988734  [ 2048/ 4760]\n",
      "loss: 0.992826  [ 2112/ 4760]\n",
      "loss: 0.988882  [ 2176/ 4760]\n",
      "loss: 0.948976  [ 2240/ 4760]\n",
      "loss: 0.940546  [ 2304/ 4760]\n",
      "loss: 1.083389  [ 2368/ 4760]\n",
      "loss: 1.037332  [ 2432/ 4760]\n",
      "loss: 0.956689  [ 2496/ 4760]\n",
      "loss: 1.011991  [ 2560/ 4760]\n",
      "loss: 1.014578  [ 2624/ 4760]\n",
      "loss: 1.013053  [ 2688/ 4760]\n",
      "loss: 1.067515  [ 2752/ 4760]\n",
      "loss: 1.034948  [ 2816/ 4760]\n",
      "loss: 0.974962  [ 2880/ 4760]\n",
      "loss: 1.015889  [ 2944/ 4760]\n",
      "loss: 1.070003  [ 3008/ 4760]\n",
      "loss: 0.956919  [ 3072/ 4760]\n",
      "loss: 1.019066  [ 3136/ 4760]\n",
      "loss: 0.984982  [ 3200/ 4760]\n",
      "loss: 1.008063  [ 3264/ 4760]\n",
      "loss: 0.995440  [ 3328/ 4760]\n",
      "loss: 0.983633  [ 3392/ 4760]\n",
      "loss: 1.008568  [ 3456/ 4760]\n",
      "loss: 0.965864  [ 3520/ 4760]\n",
      "loss: 1.071465  [ 3584/ 4760]\n",
      "loss: 0.951176  [ 3648/ 4760]\n",
      "loss: 1.045107  [ 3712/ 4760]\n",
      "loss: 1.039788  [ 3776/ 4760]\n",
      "loss: 0.995050  [ 3840/ 4760]\n",
      "loss: 1.007845  [ 3904/ 4760]\n",
      "loss: 0.938533  [ 3968/ 4760]\n",
      "loss: 0.922080  [ 4032/ 4760]\n",
      "loss: 1.001181  [ 4096/ 4760]\n",
      "loss: 0.978940  [ 4160/ 4760]\n",
      "loss: 0.974699  [ 4224/ 4760]\n",
      "loss: 0.994098  [ 4288/ 4760]\n",
      "loss: 1.002790  [ 4352/ 4760]\n",
      "loss: 0.979763  [ 4416/ 4760]\n",
      "loss: 1.057360  [ 4480/ 4760]\n",
      "loss: 0.960321  [ 4544/ 4760]\n",
      "loss: 0.962719  [ 4608/ 4760]\n",
      "loss: 1.022022  [ 4672/ 4760]\n",
      "loss: 1.026613  [ 4736/ 4760]\n",
      "loss: 1.236524  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 1.017784 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.031492  [   64/ 4760]\n",
      "loss: 0.922722  [  128/ 4760]\n",
      "loss: 1.029588  [  192/ 4760]\n",
      "loss: 1.035377  [  256/ 4760]\n",
      "loss: 0.985463  [  320/ 4760]\n",
      "loss: 1.029083  [  384/ 4760]\n",
      "loss: 0.958065  [  448/ 4760]\n",
      "loss: 1.011687  [  512/ 4760]\n",
      "loss: 0.959446  [  576/ 4760]\n",
      "loss: 0.968161  [  640/ 4760]\n",
      "loss: 0.979788  [  704/ 4760]\n",
      "loss: 1.008467  [  768/ 4760]\n",
      "loss: 1.014218  [  832/ 4760]\n",
      "loss: 0.949563  [  896/ 4760]\n",
      "loss: 0.974343  [  960/ 4760]\n",
      "loss: 0.966016  [ 1024/ 4760]\n",
      "loss: 1.058326  [ 1088/ 4760]\n",
      "loss: 0.992841  [ 1152/ 4760]\n",
      "loss: 0.943320  [ 1216/ 4760]\n",
      "loss: 0.992497  [ 1280/ 4760]\n",
      "loss: 1.174277  [ 1344/ 4760]\n",
      "loss: 0.989898  [ 1408/ 4760]\n",
      "loss: 1.044973  [ 1472/ 4760]\n",
      "loss: 0.942390  [ 1536/ 4760]\n",
      "loss: 1.000645  [ 1600/ 4760]\n",
      "loss: 1.012395  [ 1664/ 4760]\n",
      "loss: 0.983277  [ 1728/ 4760]\n",
      "loss: 1.012372  [ 1792/ 4760]\n",
      "loss: 0.938719  [ 1856/ 4760]\n",
      "loss: 0.943854  [ 1920/ 4760]\n",
      "loss: 0.932944  [ 1984/ 4760]\n",
      "loss: 0.964307  [ 2048/ 4760]\n",
      "loss: 0.991171  [ 2112/ 4760]\n",
      "loss: 0.991804  [ 2176/ 4760]\n",
      "loss: 0.979653  [ 2240/ 4760]\n",
      "loss: 1.047601  [ 2304/ 4760]\n",
      "loss: 0.982371  [ 2368/ 4760]\n",
      "loss: 0.899624  [ 2432/ 4760]\n",
      "loss: 0.971842  [ 2496/ 4760]\n",
      "loss: 0.976653  [ 2560/ 4760]\n",
      "loss: 0.935056  [ 2624/ 4760]\n",
      "loss: 0.895054  [ 2688/ 4760]\n",
      "loss: 0.975677  [ 2752/ 4760]\n",
      "loss: 1.015759  [ 2816/ 4760]\n",
      "loss: 0.891309  [ 2880/ 4760]\n",
      "loss: 1.016340  [ 2944/ 4760]\n",
      "loss: 1.001088  [ 3008/ 4760]\n",
      "loss: 0.940705  [ 3072/ 4760]\n",
      "loss: 1.008254  [ 3136/ 4760]\n",
      "loss: 0.977274  [ 3200/ 4760]\n",
      "loss: 0.994680  [ 3264/ 4760]\n",
      "loss: 0.997413  [ 3328/ 4760]\n",
      "loss: 0.984702  [ 3392/ 4760]\n",
      "loss: 0.960532  [ 3456/ 4760]\n",
      "loss: 0.937800  [ 3520/ 4760]\n",
      "loss: 0.975428  [ 3584/ 4760]\n",
      "loss: 1.011109  [ 3648/ 4760]\n",
      "loss: 1.027678  [ 3712/ 4760]\n",
      "loss: 0.970587  [ 3776/ 4760]\n",
      "loss: 1.040612  [ 3840/ 4760]\n",
      "loss: 1.000584  [ 3904/ 4760]\n",
      "loss: 1.018824  [ 3968/ 4760]\n",
      "loss: 0.924596  [ 4032/ 4760]\n",
      "loss: 0.985322  [ 4096/ 4760]\n",
      "loss: 0.894884  [ 4160/ 4760]\n",
      "loss: 1.032854  [ 4224/ 4760]\n",
      "loss: 0.930223  [ 4288/ 4760]\n",
      "loss: 0.949445  [ 4352/ 4760]\n",
      "loss: 0.983276  [ 4416/ 4760]\n",
      "loss: 0.999105  [ 4480/ 4760]\n",
      "loss: 0.996694  [ 4544/ 4760]\n",
      "loss: 0.986902  [ 4608/ 4760]\n",
      "loss: 0.929279  [ 4672/ 4760]\n",
      "loss: 0.898514  [ 4736/ 4760]\n",
      "loss: 0.950868  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 0.993230 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.030737  [   64/ 4760]\n",
      "loss: 0.933649  [  128/ 4760]\n",
      "loss: 0.926782  [  192/ 4760]\n",
      "loss: 0.952120  [  256/ 4760]\n",
      "loss: 0.959237  [  320/ 4760]\n",
      "loss: 0.987697  [  384/ 4760]\n",
      "loss: 0.890069  [  448/ 4760]\n",
      "loss: 0.937616  [  512/ 4760]\n",
      "loss: 0.980865  [  576/ 4760]\n",
      "loss: 1.010419  [  640/ 4760]\n",
      "loss: 0.906595  [  704/ 4760]\n",
      "loss: 0.936782  [  768/ 4760]\n",
      "loss: 0.915814  [  832/ 4760]\n",
      "loss: 0.973618  [  896/ 4760]\n",
      "loss: 0.978751  [  960/ 4760]\n",
      "loss: 0.951576  [ 1024/ 4760]\n",
      "loss: 0.961249  [ 1088/ 4760]\n",
      "loss: 1.030608  [ 1152/ 4760]\n",
      "loss: 0.962341  [ 1216/ 4760]\n",
      "loss: 0.894740  [ 1280/ 4760]\n",
      "loss: 0.909351  [ 1344/ 4760]\n",
      "loss: 0.934834  [ 1408/ 4760]\n",
      "loss: 0.894240  [ 1472/ 4760]\n",
      "loss: 0.947667  [ 1536/ 4760]\n",
      "loss: 0.933367  [ 1600/ 4760]\n",
      "loss: 0.921110  [ 1664/ 4760]\n",
      "loss: 0.900784  [ 1728/ 4760]\n",
      "loss: 0.936178  [ 1792/ 4760]\n",
      "loss: 0.976013  [ 1856/ 4760]\n",
      "loss: 0.985767  [ 1920/ 4760]\n",
      "loss: 0.924192  [ 1984/ 4760]\n",
      "loss: 0.934317  [ 2048/ 4760]\n",
      "loss: 0.933984  [ 2112/ 4760]\n",
      "loss: 1.001027  [ 2176/ 4760]\n",
      "loss: 1.028454  [ 2240/ 4760]\n",
      "loss: 1.016636  [ 2304/ 4760]\n",
      "loss: 0.879445  [ 2368/ 4760]\n",
      "loss: 0.868218  [ 2432/ 4760]\n",
      "loss: 0.960349  [ 2496/ 4760]\n",
      "loss: 0.920785  [ 2560/ 4760]\n",
      "loss: 0.947091  [ 2624/ 4760]\n",
      "loss: 0.951514  [ 2688/ 4760]\n",
      "loss: 0.917160  [ 2752/ 4760]\n",
      "loss: 0.892525  [ 2816/ 4760]\n",
      "loss: 0.948815  [ 2880/ 4760]\n",
      "loss: 0.956372  [ 2944/ 4760]\n",
      "loss: 1.022594  [ 3008/ 4760]\n",
      "loss: 0.958640  [ 3072/ 4760]\n",
      "loss: 1.007468  [ 3136/ 4760]\n",
      "loss: 0.908927  [ 3200/ 4760]\n",
      "loss: 1.031868  [ 3264/ 4760]\n",
      "loss: 0.969373  [ 3328/ 4760]\n",
      "loss: 0.922297  [ 3392/ 4760]\n",
      "loss: 0.913449  [ 3456/ 4760]\n",
      "loss: 0.970026  [ 3520/ 4760]\n",
      "loss: 0.964608  [ 3584/ 4760]\n",
      "loss: 0.936065  [ 3648/ 4760]\n",
      "loss: 0.901101  [ 3712/ 4760]\n",
      "loss: 0.999557  [ 3776/ 4760]\n",
      "loss: 1.002756  [ 3840/ 4760]\n",
      "loss: 0.987481  [ 3904/ 4760]\n",
      "loss: 0.948867  [ 3968/ 4760]\n",
      "loss: 0.881016  [ 4032/ 4760]\n",
      "loss: 0.926357  [ 4096/ 4760]\n",
      "loss: 0.952165  [ 4160/ 4760]\n",
      "loss: 0.935623  [ 4224/ 4760]\n",
      "loss: 0.991451  [ 4288/ 4760]\n",
      "loss: 0.967219  [ 4352/ 4760]\n",
      "loss: 0.974611  [ 4416/ 4760]\n",
      "loss: 0.939191  [ 4480/ 4760]\n",
      "loss: 0.915451  [ 4544/ 4760]\n",
      "loss: 0.914768  [ 4608/ 4760]\n",
      "loss: 0.964015  [ 4672/ 4760]\n",
      "loss: 0.980103  [ 4736/ 4760]\n",
      "loss: 0.877886  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 0.959834 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_loss</td><td>█▅▄▃▂▂▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_loss</td><td>0.95983</td></tr><tr><td>train_loss</td><td>0.87789</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">1.2</strong> at: <a href='https://wandb.ai/svenbbs/CS_challenge/runs/6n2vtvi4' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge/runs/6n2vtvi4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240320_115929-6n2vtvi4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"CS_challenge\", name=\"1.2\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": {lr_rate},\n",
    "        \"epochs\": {epochs},\n",
    "        \"Momentum\": {momentum},\n",
    "        \"Batch_size\": {batch_size},\n",
    "        \"model version\": 0.0,\n",
    "        \"subset size [%]\": 100,\n",
    "        \"resize\": {subsize},\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    run = train(train_dataloader, model, loss_fn, optimizer, run)\n",
    "    test_loss = test(val_dataloader, model, loss_fn)\n",
    "    run.log({\"Test_loss\": test_loss})\n",
    "print(\"Done!\")\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5lsm0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
