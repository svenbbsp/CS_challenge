{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Resize, Compose, v2\n",
    "from torchvision.datasets import Cityscapes\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msvenbbs\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate = 2e-3\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "subsize = (128, 256)\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:27: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:27: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\Sven\\AppData\\Local\\Temp\\ipykernel_18072\\3308899275.py:27: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  normal_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=normal_transform, target_transform=normal_target_transform)\n",
      "C:\\Users\\Sven\\AppData\\Local\\Temp\\ipykernel_18072\\3308899275.py:28: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  augmented_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=augmented_transform, target_transform=augmented_target_transform)\n",
      "c:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "augmented_transform = transforms.Compose([\n",
    "    v2.Resize(subsize),\n",
    "    v2.RandomHorizontalFlip(1),  # Random horizontal flip\n",
    "    v2.RandomRotation(10),  # Random rotation up to 10 degrees\n",
    "    v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jitter\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "augmented_target_transform = transforms.Compose([\n",
    "    v2.Resize(subsize),\n",
    "    v2.RandomHorizontalFlip(1),  # Same transformation as input images\n",
    "    v2.RandomRotation(10),  # Same transformation as input images\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "normal_transform = transforms.Compose([\n",
    "    v2.Resize(subsize),\n",
    "    v2.ToTensor()\n",
    "])\n",
    "normal_target_transform = transforms.Compose([\n",
    "    v2.Resize(subsize),\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "#Desktop\n",
    "normal_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=normal_transform, target_transform=normal_target_transform)\n",
    "augmented_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=augmented_transform, target_transform=augmented_target_transform)\n",
    "dataset = torch.utils.data.ConcatDataset([normal_dataset, augmented_dataset])\n",
    "\n",
    "\n",
    "#Laptop\n",
    "#dataset = Cityscapes(root=\"C:/Users/20182573/Documents/CityScapes\", split='train', mode='fine', target_type='semantic', transform=transform, target_transform=target_transform)\n",
    "\n",
    "#subset_small, subset_big = random_split(dataset, [0.2,0.8])\n",
    "train_dataset, val_dataset = random_split(dataset, [0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4760"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from model import Model\n",
    "#model = Model().cuda()\n",
    "\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False, progress=True, num_classes=34).cuda()\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, run):\n",
    "    \"\"\"\n",
    "    Train a model for 1 epoch.\n",
    "\n",
    "    Params:\n",
    "    - dataloader:   dataset to train on.\n",
    "    - model:        the model object to be trained.\n",
    "    - loss_fn:      the loss function.\n",
    "    - optimizer:    the desired optimization.\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() #Set the model to train mode\n",
    "    for batch, (IMG,SEGM) in enumerate(dataloader):\n",
    "        IMG = IMG.to('cuda')\n",
    "        SEGM  = (SEGM*255).long().squeeze()     #*255 because the id are normalized between 0-1\n",
    "        SEGM = utils.map_id_to_train_id(SEGM).to('cuda')\n",
    "        \n",
    "        #predict\n",
    "        pred = model(IMG)['out']\n",
    "        #Loss\n",
    "        loss = loss_fn(pred, SEGM)\n",
    "        \n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #print loss during training\n",
    "        loss, current = loss.item(), (batch + 1) * len(IMG)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        run.log({\"train_loss\": loss})\n",
    "    \n",
    "    return run\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Test a model.\n",
    "\n",
    "    Params:\n",
    "    - dataloader:   dataset to test on.\n",
    "    - model:        the model object to be tested.\n",
    "    - loss_fn:      the loss function.\n",
    "    \"\"\"\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() #model in eval mode\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (IMG,SEGM) in enumerate(dataloader):\n",
    "            IMG = IMG.to('cuda')\n",
    "            SEGM  = (SEGM*255).long().squeeze()     #*255 because the id are normalized between 0-1\n",
    "            SEGM = utils.map_id_to_train_id(SEGM).to('cuda')\n",
    "\n",
    "            pred = model(IMG)['out']\n",
    "            test_loss += loss_fn(pred, SEGM).item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Sven\\Documents\\GitHub\\CS_challenge\\wandb\\run-20240320_235629-oncyixol</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/svenbbs/CS_challenge/runs/oncyixol' target=\"_blank\">2.0</a></strong> to <a href='https://wandb.ai/svenbbs/CS_challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/svenbbs/CS_challenge' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/svenbbs/CS_challenge/runs/oncyixol' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge/runs/oncyixol</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.644441  [   64/ 4760]\n",
      "loss: 3.621849  [  128/ 4760]\n",
      "loss: 3.567999  [  192/ 4760]\n",
      "loss: 3.520430  [  256/ 4760]\n",
      "loss: 3.426957  [  320/ 4760]\n",
      "loss: 3.336647  [  384/ 4760]\n",
      "loss: 3.231122  [  448/ 4760]\n",
      "loss: 3.137012  [  512/ 4760]\n",
      "loss: 3.007363  [  576/ 4760]\n",
      "loss: 2.866226  [  640/ 4760]\n",
      "loss: 2.756863  [  704/ 4760]\n",
      "loss: 2.596910  [  768/ 4760]\n",
      "loss: 2.479400  [  832/ 4760]\n",
      "loss: 2.401435  [  896/ 4760]\n",
      "loss: 2.266798  [  960/ 4760]\n",
      "loss: 2.142769  [ 1024/ 4760]\n",
      "loss: 2.027126  [ 1088/ 4760]\n",
      "loss: 1.952096  [ 1152/ 4760]\n",
      "loss: 1.864465  [ 1216/ 4760]\n",
      "loss: 1.772544  [ 1280/ 4760]\n",
      "loss: 1.702581  [ 1344/ 4760]\n",
      "loss: 1.717569  [ 1408/ 4760]\n",
      "loss: 1.674434  [ 1472/ 4760]\n",
      "loss: 1.503769  [ 1536/ 4760]\n",
      "loss: 1.527136  [ 1600/ 4760]\n",
      "loss: 1.537006  [ 1664/ 4760]\n",
      "loss: 1.471014  [ 1728/ 4760]\n",
      "loss: 1.409607  [ 1792/ 4760]\n",
      "loss: 1.443242  [ 1856/ 4760]\n",
      "loss: 1.468837  [ 1920/ 4760]\n",
      "loss: 1.474934  [ 1984/ 4760]\n",
      "loss: 1.490449  [ 2048/ 4760]\n",
      "loss: 1.428401  [ 2112/ 4760]\n",
      "loss: 1.402361  [ 2176/ 4760]\n",
      "loss: 1.420052  [ 2240/ 4760]\n",
      "loss: 1.294652  [ 2304/ 4760]\n",
      "loss: 1.334748  [ 2368/ 4760]\n",
      "loss: 1.341878  [ 2432/ 4760]\n",
      "loss: 1.361374  [ 2496/ 4760]\n",
      "loss: 1.344099  [ 2560/ 4760]\n",
      "loss: 1.309627  [ 2624/ 4760]\n",
      "loss: 1.297109  [ 2688/ 4760]\n",
      "loss: 1.229254  [ 2752/ 4760]\n",
      "loss: 1.231158  [ 2816/ 4760]\n",
      "loss: 1.336453  [ 2880/ 4760]\n",
      "loss: 1.239194  [ 2944/ 4760]\n",
      "loss: 1.213773  [ 3008/ 4760]\n",
      "loss: 1.246418  [ 3072/ 4760]\n",
      "loss: 1.191325  [ 3136/ 4760]\n",
      "loss: 1.195652  [ 3200/ 4760]\n",
      "loss: 1.226339  [ 3264/ 4760]\n",
      "loss: 1.184962  [ 3328/ 4760]\n",
      "loss: 1.172972  [ 3392/ 4760]\n",
      "loss: 1.190008  [ 3456/ 4760]\n",
      "loss: 1.163451  [ 3520/ 4760]\n",
      "loss: 1.168583  [ 3584/ 4760]\n",
      "loss: 1.066120  [ 3648/ 4760]\n",
      "loss: 1.096379  [ 3712/ 4760]\n",
      "loss: 1.090663  [ 3776/ 4760]\n",
      "loss: 1.106349  [ 3840/ 4760]\n",
      "loss: 1.136462  [ 3904/ 4760]\n",
      "loss: 1.101538  [ 3968/ 4760]\n",
      "loss: 1.096555  [ 4032/ 4760]\n",
      "loss: 1.035891  [ 4096/ 4760]\n",
      "loss: 1.154286  [ 4160/ 4760]\n",
      "loss: 1.073069  [ 4224/ 4760]\n",
      "loss: 1.096342  [ 4288/ 4760]\n",
      "loss: 1.072586  [ 4352/ 4760]\n",
      "loss: 1.125388  [ 4416/ 4760]\n",
      "loss: 1.074089  [ 4480/ 4760]\n",
      "loss: 1.023391  [ 4544/ 4760]\n",
      "loss: 1.055181  [ 4608/ 4760]\n",
      "loss: 1.060394  [ 4672/ 4760]\n",
      "loss: 1.057051  [ 4736/ 4760]\n",
      "loss: 1.150709  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 1.059105 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.983659  [   64/ 4760]\n",
      "loss: 0.971933  [  128/ 4760]\n",
      "loss: 1.021686  [  192/ 4760]\n",
      "loss: 1.000139  [  256/ 4760]\n",
      "loss: 1.042509  [  320/ 4760]\n",
      "loss: 1.064827  [  384/ 4760]\n",
      "loss: 1.095138  [  448/ 4760]\n",
      "loss: 1.004575  [  512/ 4760]\n",
      "loss: 1.010498  [  576/ 4760]\n",
      "loss: 1.012608  [  640/ 4760]\n",
      "loss: 1.027524  [  704/ 4760]\n",
      "loss: 0.956846  [  768/ 4760]\n",
      "loss: 1.053994  [  832/ 4760]\n",
      "loss: 1.071462  [  896/ 4760]\n",
      "loss: 1.034117  [  960/ 4760]\n",
      "loss: 1.039277  [ 1024/ 4760]\n",
      "loss: 0.933060  [ 1088/ 4760]\n",
      "loss: 0.986517  [ 1152/ 4760]\n",
      "loss: 0.996482  [ 1216/ 4760]\n",
      "loss: 0.961971  [ 1280/ 4760]\n",
      "loss: 0.996858  [ 1344/ 4760]\n",
      "loss: 0.979088  [ 1408/ 4760]\n",
      "loss: 0.910554  [ 1472/ 4760]\n",
      "loss: 0.985790  [ 1536/ 4760]\n",
      "loss: 1.033872  [ 1600/ 4760]\n",
      "loss: 0.991014  [ 1664/ 4760]\n",
      "loss: 1.004638  [ 1728/ 4760]\n",
      "loss: 1.006898  [ 1792/ 4760]\n",
      "loss: 1.021639  [ 1856/ 4760]\n",
      "loss: 0.970869  [ 1920/ 4760]\n",
      "loss: 1.009382  [ 1984/ 4760]\n",
      "loss: 0.956046  [ 2048/ 4760]\n",
      "loss: 0.954049  [ 2112/ 4760]\n",
      "loss: 1.003058  [ 2176/ 4760]\n",
      "loss: 1.001738  [ 2240/ 4760]\n",
      "loss: 0.955941  [ 2304/ 4760]\n",
      "loss: 1.008998  [ 2368/ 4760]\n",
      "loss: 0.994025  [ 2432/ 4760]\n",
      "loss: 0.919167  [ 2496/ 4760]\n",
      "loss: 1.032631  [ 2560/ 4760]\n",
      "loss: 1.001964  [ 2624/ 4760]\n",
      "loss: 1.024858  [ 2688/ 4760]\n",
      "loss: 0.947053  [ 2752/ 4760]\n",
      "loss: 0.937204  [ 2816/ 4760]\n",
      "loss: 0.921478  [ 2880/ 4760]\n",
      "loss: 0.975230  [ 2944/ 4760]\n",
      "loss: 0.956438  [ 3008/ 4760]\n",
      "loss: 1.015014  [ 3072/ 4760]\n",
      "loss: 0.895374  [ 3136/ 4760]\n",
      "loss: 0.910173  [ 3200/ 4760]\n",
      "loss: 0.917748  [ 3264/ 4760]\n",
      "loss: 0.914407  [ 3328/ 4760]\n",
      "loss: 0.911712  [ 3392/ 4760]\n",
      "loss: 0.897608  [ 3456/ 4760]\n",
      "loss: 0.958389  [ 3520/ 4760]\n",
      "loss: 0.965096  [ 3584/ 4760]\n",
      "loss: 0.943193  [ 3648/ 4760]\n",
      "loss: 0.929603  [ 3712/ 4760]\n",
      "loss: 0.926680  [ 3776/ 4760]\n",
      "loss: 0.915420  [ 3840/ 4760]\n",
      "loss: 0.903573  [ 3904/ 4760]\n",
      "loss: 0.891396  [ 3968/ 4760]\n",
      "loss: 0.898168  [ 4032/ 4760]\n",
      "loss: 0.998087  [ 4096/ 4760]\n",
      "loss: 1.012287  [ 4160/ 4760]\n",
      "loss: 0.942136  [ 4224/ 4760]\n",
      "loss: 0.872846  [ 4288/ 4760]\n",
      "loss: 0.916484  [ 4352/ 4760]\n",
      "loss: 0.933657  [ 4416/ 4760]\n",
      "loss: 0.880973  [ 4480/ 4760]\n",
      "loss: 0.887942  [ 4544/ 4760]\n",
      "loss: 0.896688  [ 4608/ 4760]\n",
      "loss: 0.909270  [ 4672/ 4760]\n",
      "loss: 0.945297  [ 4736/ 4760]\n",
      "loss: 0.962897  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 0.913822 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.904331  [   64/ 4760]\n",
      "loss: 0.945134  [  128/ 4760]\n",
      "loss: 0.901422  [  192/ 4760]\n",
      "loss: 0.879178  [  256/ 4760]\n",
      "loss: 0.912332  [  320/ 4760]\n",
      "loss: 0.885317  [  384/ 4760]\n",
      "loss: 0.899464  [  448/ 4760]\n",
      "loss: 0.897668  [  512/ 4760]\n",
      "loss: 0.904759  [  576/ 4760]\n",
      "loss: 0.936552  [  640/ 4760]\n",
      "loss: 0.910081  [  704/ 4760]\n",
      "loss: 0.912011  [  768/ 4760]\n",
      "loss: 0.890520  [  832/ 4760]\n",
      "loss: 0.917109  [  896/ 4760]\n",
      "loss: 0.919320  [  960/ 4760]\n",
      "loss: 0.864151  [ 1024/ 4760]\n",
      "loss: 0.917778  [ 1088/ 4760]\n",
      "loss: 0.966245  [ 1152/ 4760]\n",
      "loss: 0.893703  [ 1216/ 4760]\n",
      "loss: 0.886857  [ 1280/ 4760]\n",
      "loss: 0.871041  [ 1344/ 4760]\n",
      "loss: 0.877017  [ 1408/ 4760]\n",
      "loss: 0.858462  [ 1472/ 4760]\n",
      "loss: 0.884287  [ 1536/ 4760]\n",
      "loss: 0.827963  [ 1600/ 4760]\n",
      "loss: 0.918266  [ 1664/ 4760]\n",
      "loss: 0.851680  [ 1728/ 4760]\n",
      "loss: 0.909201  [ 1792/ 4760]\n",
      "loss: 0.861270  [ 1856/ 4760]\n",
      "loss: 0.910240  [ 1920/ 4760]\n",
      "loss: 0.849165  [ 1984/ 4760]\n",
      "loss: 0.902600  [ 2048/ 4760]\n",
      "loss: 0.889615  [ 2112/ 4760]\n",
      "loss: 0.890685  [ 2176/ 4760]\n",
      "loss: 0.891793  [ 2240/ 4760]\n",
      "loss: 0.872602  [ 2304/ 4760]\n",
      "loss: 0.904115  [ 2368/ 4760]\n",
      "loss: 0.859171  [ 2432/ 4760]\n",
      "loss: 0.827171  [ 2496/ 4760]\n",
      "loss: 0.900098  [ 2560/ 4760]\n",
      "loss: 0.872002  [ 2624/ 4760]\n",
      "loss: 0.856896  [ 2688/ 4760]\n",
      "loss: 0.891826  [ 2752/ 4760]\n",
      "loss: 0.853294  [ 2816/ 4760]\n",
      "loss: 0.890243  [ 2880/ 4760]\n",
      "loss: 0.843749  [ 2944/ 4760]\n",
      "loss: 0.913692  [ 3008/ 4760]\n",
      "loss: 0.892000  [ 3072/ 4760]\n",
      "loss: 0.893110  [ 3136/ 4760]\n",
      "loss: 0.834198  [ 3200/ 4760]\n",
      "loss: 0.866864  [ 3264/ 4760]\n",
      "loss: 0.843785  [ 3328/ 4760]\n",
      "loss: 0.935279  [ 3392/ 4760]\n",
      "loss: 0.918434  [ 3456/ 4760]\n",
      "loss: 0.852736  [ 3520/ 4760]\n",
      "loss: 0.844314  [ 3584/ 4760]\n",
      "loss: 0.874128  [ 3648/ 4760]\n",
      "loss: 0.886167  [ 3712/ 4760]\n",
      "loss: 0.886760  [ 3776/ 4760]\n",
      "loss: 0.814928  [ 3840/ 4760]\n",
      "loss: 0.859000  [ 3904/ 4760]\n",
      "loss: 0.884413  [ 3968/ 4760]\n",
      "loss: 0.926299  [ 4032/ 4760]\n",
      "loss: 0.838400  [ 4096/ 4760]\n",
      "loss: 0.830181  [ 4160/ 4760]\n",
      "loss: 0.823604  [ 4224/ 4760]\n",
      "loss: 0.926601  [ 4288/ 4760]\n",
      "loss: 0.870123  [ 4352/ 4760]\n",
      "loss: 0.832789  [ 4416/ 4760]\n",
      "loss: 0.887394  [ 4480/ 4760]\n",
      "loss: 0.801622  [ 4544/ 4760]\n",
      "loss: 0.929323  [ 4608/ 4760]\n",
      "loss: 0.841194  [ 4672/ 4760]\n",
      "loss: 0.874829  [ 4736/ 4760]\n",
      "loss: 0.867828  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 0.853242 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.803206  [   64/ 4760]\n",
      "loss: 0.822507  [  128/ 4760]\n",
      "loss: 0.806322  [  192/ 4760]\n",
      "loss: 0.877262  [  256/ 4760]\n",
      "loss: 0.860512  [  320/ 4760]\n",
      "loss: 0.831467  [  384/ 4760]\n",
      "loss: 0.764171  [  448/ 4760]\n",
      "loss: 0.841507  [  512/ 4760]\n",
      "loss: 0.808957  [  576/ 4760]\n",
      "loss: 0.883253  [  640/ 4760]\n",
      "loss: 0.855560  [  704/ 4760]\n",
      "loss: 0.894215  [  768/ 4760]\n",
      "loss: 0.871515  [  832/ 4760]\n",
      "loss: 0.884467  [  896/ 4760]\n",
      "loss: 0.842067  [  960/ 4760]\n",
      "loss: 0.833905  [ 1024/ 4760]\n",
      "loss: 0.863435  [ 1088/ 4760]\n",
      "loss: 0.863272  [ 1152/ 4760]\n",
      "loss: 0.829754  [ 1216/ 4760]\n",
      "loss: 0.817873  [ 1280/ 4760]\n",
      "loss: 0.841160  [ 1344/ 4760]\n",
      "loss: 0.847754  [ 1408/ 4760]\n",
      "loss: 0.916483  [ 1472/ 4760]\n",
      "loss: 0.793751  [ 1536/ 4760]\n",
      "loss: 0.834323  [ 1600/ 4760]\n",
      "loss: 0.823405  [ 1664/ 4760]\n",
      "loss: 0.755776  [ 1728/ 4760]\n",
      "loss: 0.846095  [ 1792/ 4760]\n",
      "loss: 0.816453  [ 1856/ 4760]\n",
      "loss: 0.903584  [ 1920/ 4760]\n",
      "loss: 0.838906  [ 1984/ 4760]\n",
      "loss: 0.824447  [ 2048/ 4760]\n",
      "loss: 0.814773  [ 2112/ 4760]\n",
      "loss: 0.819498  [ 2176/ 4760]\n",
      "loss: 0.895805  [ 2240/ 4760]\n",
      "loss: 0.822517  [ 2304/ 4760]\n",
      "loss: 0.839011  [ 2368/ 4760]\n",
      "loss: 0.794515  [ 2432/ 4760]\n",
      "loss: 0.872579  [ 2496/ 4760]\n",
      "loss: 0.801939  [ 2560/ 4760]\n",
      "loss: 0.804372  [ 2624/ 4760]\n",
      "loss: 0.825950  [ 2688/ 4760]\n",
      "loss: 0.832362  [ 2752/ 4760]\n",
      "loss: 0.791159  [ 2816/ 4760]\n",
      "loss: 0.878320  [ 2880/ 4760]\n",
      "loss: 0.841487  [ 2944/ 4760]\n",
      "loss: 0.813441  [ 3008/ 4760]\n",
      "loss: 0.893394  [ 3072/ 4760]\n",
      "loss: 0.867812  [ 3136/ 4760]\n",
      "loss: 0.783503  [ 3200/ 4760]\n",
      "loss: 0.788509  [ 3264/ 4760]\n",
      "loss: 0.838073  [ 3328/ 4760]\n",
      "loss: 0.853816  [ 3392/ 4760]\n",
      "loss: 0.879661  [ 3456/ 4760]\n",
      "loss: 0.825394  [ 3520/ 4760]\n",
      "loss: 0.794842  [ 3584/ 4760]\n",
      "loss: 0.794082  [ 3648/ 4760]\n",
      "loss: 0.796725  [ 3712/ 4760]\n",
      "loss: 0.824071  [ 3776/ 4760]\n",
      "loss: 0.796879  [ 3840/ 4760]\n",
      "loss: 0.778542  [ 3904/ 4760]\n",
      "loss: 0.781973  [ 3968/ 4760]\n",
      "loss: 0.879555  [ 4032/ 4760]\n",
      "loss: 0.843696  [ 4096/ 4760]\n",
      "loss: 0.896083  [ 4160/ 4760]\n",
      "loss: 0.814759  [ 4224/ 4760]\n",
      "loss: 0.789240  [ 4288/ 4760]\n",
      "loss: 0.782621  [ 4352/ 4760]\n",
      "loss: 0.853819  [ 4416/ 4760]\n",
      "loss: 0.791751  [ 4480/ 4760]\n",
      "loss: 0.839433  [ 4544/ 4760]\n",
      "loss: 0.798090  [ 4608/ 4760]\n",
      "loss: 0.802205  [ 4672/ 4760]\n",
      "loss: 0.864643  [ 4736/ 4760]\n",
      "loss: 0.846742  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 0.815936 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m     run \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test(val_dataloader, model, loss_fn)\n\u001b[0;32m     23\u001b[0m     run\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: t})\n",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer, run)\u001b[0m\n\u001b[0;32m     11\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m#Set the model to train mode\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (IMG,SEGM) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     14\u001b[0m     IMG \u001b[38;5;241m=\u001b[39m IMG\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m     SEGM  \u001b[38;5;241m=\u001b[39m (SEGM\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39msqueeze()     \u001b[38;5;66;03m#*255 because the id are normalized between 0-1\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"CS_challenge\", name=\"2.0\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": {lr_rate},\n",
    "        \"epochs\": {epochs},\n",
    "        \"Momentum\": {momentum},\n",
    "        \"Batch_size\": {batch_size},\n",
    "        \"model version\": 1.0,\n",
    "        \"subset size [%]\": 100,\n",
    "        \"resize\": {subsize},\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    run = train(train_dataloader, model, loss_fn, optimizer, run)\n",
    "    test_loss = test(val_dataloader, model, loss_fn)\n",
    "    run.log({\"Test_loss\": test_loss, \"Epoch\": t})\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▆█</td></tr><tr><td>Test_loss</td><td>█▄▂▁</td></tr><tr><td>train_loss</td><td>█▇▅▃▃▂▂▂▂▂▂▂▁▂▂▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>3</td></tr><tr><td>Test_loss</td><td>0.81594</td></tr><tr><td>train_loss</td><td>0.84674</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2.0</strong> at: <a href='https://wandb.ai/svenbbs/CS_challenge/runs/oncyixol' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge/runs/oncyixol</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240320_235629-oncyixol\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5lsm0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
