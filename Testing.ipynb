{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Resize, Compose, v2\n",
    "from torchvision.datasets import Cityscapes\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msvenbbs\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate = 1e-3\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "subsize = (128, 256)\n",
    "momentum = 0.9\n",
    "mean, std = ([0.2776, 0.3131, 0.2740]), ([0.1695, 0.1745, 0.1706])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:28: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\Sven\\AppData\\Local\\Temp\\ipykernel_18388\\3415046684.py:28: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  normal_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=normal_transform, target_transform=normal_target_transform)\n",
      "C:\\Users\\Sven\\AppData\\Local\\Temp\\ipykernel_18388\\3415046684.py:29: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  augmented_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=augmented_transform, target_transform=augmented_target_transform)\n",
      "c:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "augmented_transform = transforms.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize(subsize),\n",
    "    v2.RandomHorizontalFlip(1),  # Random horizontal flip\n",
    "    #v2.RandomRotation(10),  # Random rotation up to 10 degrees\n",
    "    #v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jitter\n",
    "    \n",
    "])\n",
    "\n",
    "augmented_target_transform = transforms.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize(subsize),\n",
    "    #v2.RandomHorizontalFlip(1),  # Same transformation as input images\n",
    "    #v2.RandomRotation(10),  # Same transformation as input images\n",
    "])\n",
    "\n",
    "normal_transform = transforms.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize(subsize),\n",
    "])\n",
    "normal_target_transform = transforms.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize(subsize),\n",
    "])\n",
    "\n",
    "\n",
    "#Desktop\n",
    "normal_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=normal_transform, target_transform=normal_target_transform)\n",
    "augmented_dataset = Cityscapes(root=\"E:\\CityScapes\", split='train', mode='fine', target_type='semantic', transform=augmented_transform, target_transform=augmented_target_transform)\n",
    "dataset = torch.utils.data.ConcatDataset([normal_dataset, augmented_dataset])\n",
    "\n",
    "\n",
    "#Laptop\n",
    "#dataset = Cityscapes(root=\"C:/Users/20182573/Documents/CityScapes\", split='train', mode='fine', target_type='semantic', transform=transform, target_transform=target_transform)\n",
    "\n",
    "#subset_small, subset_big = random_split(dataset, [0.2,0.8])\n",
    "train_dataset, val_dataset = random_split(dataset, [0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_std(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "\n",
    "    mean /= len(loader.dataset)\n",
    "    std /= len(loader.dataset)\n",
    "    return mean, std\n",
    "\n",
    "#mean, std = mean_and_std(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model import Model\n",
    "model = Model().cuda()\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, run):\n",
    "    \"\"\"\n",
    "    Train a model for 1 epoch.\n",
    "\n",
    "    Params:\n",
    "    - dataloader:   dataset to train on.\n",
    "    - model:        the model object to be trained.\n",
    "    - loss_fn:      the loss function.\n",
    "    - optimizer:    the desired optimization.\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() #Set the model to train mode\n",
    "    for batch, (IMG,SEGM) in enumerate(dataloader):\n",
    "        IMG = IMG.to('cuda')\n",
    "        SEGM  = (SEGM*255).long().squeeze()     #*255 because the id are normalized between 0-1\n",
    "        SEGM = utils.map_id_to_train_id(SEGM).to('cuda')\n",
    "        \n",
    "        #predict\n",
    "        pred = model(IMG)\n",
    "        #Loss\n",
    "        loss = loss_fn(pred, SEGM)\n",
    "        \n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #print loss during training\n",
    "        loss, current = loss.item(), (batch + 1) * len(IMG)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        run.log({\"train_loss\": loss})\n",
    "    \n",
    "    return run\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Test a model.\n",
    "\n",
    "    Params:\n",
    "    - dataloader:   dataset to test on.\n",
    "    - model:        the model object to be tested.\n",
    "    - loss_fn:      the loss function.\n",
    "    \"\"\"\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() #model in eval mode\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (IMG,SEGM) in enumerate(dataloader):\n",
    "            IMG = IMG.to('cuda')\n",
    "            SEGM  = (SEGM*255).long().squeeze()     #*255 because the id are normalized between 0-1\n",
    "            SEGM = utils.map_id_to_train_id(SEGM).to('cuda')\n",
    "\n",
    "            pred = model(IMG)\n",
    "            test_loss += loss_fn(pred, SEGM).item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Sven\\Documents\\GitHub\\CS_challenge\\wandb\\run-20240326_152022-6wuzt3f5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/svenbbs/CS_challenge/runs/6wuzt3f5' target=\"_blank\">1.7</a></strong> to <a href='https://wandb.ai/svenbbs/CS_challenge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/svenbbs/CS_challenge' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/svenbbs/CS_challenge/runs/6wuzt3f5' target=\"_blank\">https://wandb.ai/svenbbs/CS_challenge/runs/6wuzt3f5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.534487  [   64/ 4760]\n",
      "loss: 3.180811  [  128/ 4760]\n",
      "loss: 2.953079  [  192/ 4760]\n",
      "loss: 2.812852  [  256/ 4760]\n",
      "loss: 2.636143  [  320/ 4760]\n",
      "loss: 2.513393  [  384/ 4760]\n",
      "loss: 2.441398  [  448/ 4760]\n",
      "loss: 2.337048  [  512/ 4760]\n",
      "loss: 2.282129  [  576/ 4760]\n",
      "loss: 2.223753  [  640/ 4760]\n",
      "loss: 2.131212  [  704/ 4760]\n",
      "loss: 2.099603  [  768/ 4760]\n",
      "loss: 2.055564  [  832/ 4760]\n",
      "loss: 1.993006  [  896/ 4760]\n",
      "loss: 1.916375  [  960/ 4760]\n",
      "loss: 1.874338  [ 1024/ 4760]\n",
      "loss: 1.885856  [ 1088/ 4760]\n",
      "loss: 1.806984  [ 1152/ 4760]\n",
      "loss: 1.783181  [ 1216/ 4760]\n",
      "loss: 1.744170  [ 1280/ 4760]\n",
      "loss: 1.643790  [ 1344/ 4760]\n",
      "loss: 1.601516  [ 1408/ 4760]\n",
      "loss: 1.682422  [ 1472/ 4760]\n",
      "loss: 1.635722  [ 1536/ 4760]\n",
      "loss: 1.640763  [ 1600/ 4760]\n",
      "loss: 1.578774  [ 1664/ 4760]\n",
      "loss: 1.576934  [ 1728/ 4760]\n",
      "loss: 1.538259  [ 1792/ 4760]\n",
      "loss: 1.568531  [ 1856/ 4760]\n",
      "loss: 1.516172  [ 1920/ 4760]\n",
      "loss: 1.458884  [ 1984/ 4760]\n",
      "loss: 1.430593  [ 2048/ 4760]\n",
      "loss: 1.492693  [ 2112/ 4760]\n",
      "loss: 1.446275  [ 2176/ 4760]\n",
      "loss: 1.417969  [ 2240/ 4760]\n",
      "loss: 1.375842  [ 2304/ 4760]\n",
      "loss: 1.370705  [ 2368/ 4760]\n",
      "loss: 1.351865  [ 2432/ 4760]\n",
      "loss: 1.391169  [ 2496/ 4760]\n",
      "loss: 1.357004  [ 2560/ 4760]\n",
      "loss: 1.379537  [ 2624/ 4760]\n",
      "loss: 1.328459  [ 2688/ 4760]\n",
      "loss: 1.315470  [ 2752/ 4760]\n",
      "loss: 1.289696  [ 2816/ 4760]\n",
      "loss: 1.197610  [ 2880/ 4760]\n",
      "loss: 1.266384  [ 2944/ 4760]\n",
      "loss: 1.221207  [ 3008/ 4760]\n",
      "loss: 1.273265  [ 3072/ 4760]\n",
      "loss: 1.230724  [ 3136/ 4760]\n",
      "loss: 1.287614  [ 3200/ 4760]\n",
      "loss: 1.256191  [ 3264/ 4760]\n",
      "loss: 1.376502  [ 3328/ 4760]\n",
      "loss: 1.305461  [ 3392/ 4760]\n",
      "loss: 1.191269  [ 3456/ 4760]\n",
      "loss: 1.209618  [ 3520/ 4760]\n",
      "loss: 1.229412  [ 3584/ 4760]\n",
      "loss: 1.268453  [ 3648/ 4760]\n",
      "loss: 1.185839  [ 3712/ 4760]\n",
      "loss: 1.182481  [ 3776/ 4760]\n",
      "loss: 1.159714  [ 3840/ 4760]\n",
      "loss: 1.283315  [ 3904/ 4760]\n",
      "loss: 1.191575  [ 3968/ 4760]\n",
      "loss: 1.251602  [ 4032/ 4760]\n",
      "loss: 1.177951  [ 4096/ 4760]\n",
      "loss: 1.112467  [ 4160/ 4760]\n",
      "loss: 1.135995  [ 4224/ 4760]\n",
      "loss: 1.203791  [ 4288/ 4760]\n",
      "loss: 1.212090  [ 4352/ 4760]\n",
      "loss: 1.095816  [ 4416/ 4760]\n",
      "loss: 1.112302  [ 4480/ 4760]\n",
      "loss: 1.160503  [ 4544/ 4760]\n",
      "loss: 1.151347  [ 4608/ 4760]\n",
      "loss: 1.084326  [ 4672/ 4760]\n",
      "loss: 1.151442  [ 4736/ 4760]\n",
      "loss: 1.026066  [ 1800/ 4760]\n",
      "Test Error: \n",
      " Avg loss: 1.172287 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.127527  [   64/ 4760]\n",
      "loss: 1.176614  [  128/ 4760]\n",
      "loss: 1.148513  [  192/ 4760]\n",
      "loss: 1.137977  [  256/ 4760]\n",
      "loss: 1.133289  [  320/ 4760]\n",
      "loss: 1.104797  [  384/ 4760]\n",
      "loss: 1.209832  [  448/ 4760]\n",
      "loss: 1.210560  [  512/ 4760]\n",
      "loss: 1.065580  [  576/ 4760]\n",
      "loss: 1.074297  [  640/ 4760]\n",
      "loss: 1.148922  [  704/ 4760]\n",
      "loss: 1.095443  [  768/ 4760]\n",
      "loss: 1.145213  [  832/ 4760]\n",
      "loss: 1.071237  [  896/ 4760]\n",
      "loss: 1.072580  [  960/ 4760]\n",
      "loss: 1.098389  [ 1024/ 4760]\n",
      "loss: 1.110700  [ 1088/ 4760]\n",
      "loss: 1.020041  [ 1152/ 4760]\n",
      "loss: 1.040472  [ 1216/ 4760]\n",
      "loss: 1.057256  [ 1280/ 4760]\n",
      "loss: 1.169718  [ 1344/ 4760]\n",
      "loss: 1.057819  [ 1408/ 4760]\n",
      "loss: 1.144969  [ 1472/ 4760]\n",
      "loss: 1.125176  [ 1536/ 4760]\n",
      "loss: 1.026862  [ 1600/ 4760]\n",
      "loss: 1.095903  [ 1664/ 4760]\n",
      "loss: 1.058677  [ 1728/ 4760]\n",
      "loss: 1.048028  [ 1792/ 4760]\n",
      "loss: 1.063727  [ 1856/ 4760]\n",
      "loss: 1.095649  [ 1920/ 4760]\n",
      "loss: 1.133904  [ 1984/ 4760]\n",
      "loss: 1.065442  [ 2048/ 4760]\n",
      "loss: 1.064868  [ 2112/ 4760]\n",
      "loss: 1.073632  [ 2176/ 4760]\n",
      "loss: 1.078426  [ 2240/ 4760]\n",
      "loss: 1.118918  [ 2304/ 4760]\n",
      "loss: 1.074445  [ 2368/ 4760]\n",
      "loss: 1.116259  [ 2432/ 4760]\n",
      "loss: 1.000123  [ 2496/ 4760]\n",
      "loss: 0.983473  [ 2560/ 4760]\n",
      "loss: 0.966593  [ 2624/ 4760]\n",
      "loss: 1.083941  [ 2688/ 4760]\n",
      "loss: 1.042804  [ 2752/ 4760]\n",
      "loss: 1.117564  [ 2816/ 4760]\n",
      "loss: 1.119669  [ 2880/ 4760]\n",
      "loss: 1.085496  [ 2944/ 4760]\n",
      "loss: 1.093725  [ 3008/ 4760]\n",
      "loss: 1.132350  [ 3072/ 4760]\n",
      "loss: 0.965974  [ 3136/ 4760]\n",
      "loss: 1.064018  [ 3200/ 4760]\n",
      "loss: 1.058172  [ 3264/ 4760]\n",
      "loss: 1.067039  [ 3328/ 4760]\n",
      "loss: 1.087114  [ 3392/ 4760]\n",
      "loss: 0.997743  [ 3456/ 4760]\n",
      "loss: 1.031096  [ 3520/ 4760]\n",
      "loss: 0.958980  [ 3584/ 4760]\n",
      "loss: 1.017790  [ 3648/ 4760]\n",
      "loss: 1.046424  [ 3712/ 4760]\n",
      "loss: 1.034052  [ 3776/ 4760]\n",
      "loss: 1.065761  [ 3840/ 4760]\n",
      "loss: 1.047279  [ 3904/ 4760]\n",
      "loss: 1.028297  [ 3968/ 4760]\n",
      "loss: 1.077521  [ 4032/ 4760]\n",
      "loss: 1.068382  [ 4096/ 4760]\n",
      "loss: 1.059951  [ 4160/ 4760]\n",
      "loss: 1.059393  [ 4224/ 4760]\n",
      "loss: 1.022458  [ 4288/ 4760]\n",
      "loss: 1.061661  [ 4352/ 4760]\n",
      "loss: 1.093375  [ 4416/ 4760]\n",
      "loss: 0.968942  [ 4480/ 4760]\n",
      "loss: 1.045758  [ 4544/ 4760]\n",
      "loss: 1.001575  [ 4608/ 4760]\n",
      "loss: 1.121265  [ 4672/ 4760]\n",
      "loss: 1.026886  [ 4736/ 4760]\n",
      "loss: 0.999021  [ 1800/ 4760]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m     run \u001b[38;5;241m=\u001b[39m train(train_dataloader, model, loss_fn, optimizer, run)\n\u001b[1;32m---> 24\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     run\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: t})\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[0;32m     12\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMG\u001b[49m\u001b[43m,\u001b[49m\u001b[43mSEGM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mIMG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mSEGM\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mSEGM\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m#*255 because the id are normalized between 0-1\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:335\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\datasets\\cityscapes.py:183\u001b[0m, in \u001b[0;36mCityscapes.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m    175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m        index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m        than one item. Otherwise, target is a json object if target_type=\"polygon\", else the image segmentation.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m     targets: Any \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_type):\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\PIL\\Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    876\u001b[0m ):\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sven\\anaconda3\\envs\\torch\\Lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"CS_challenge\", name=\"1.7\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": {lr_rate},\n",
    "        \"epochs\": {epochs},\n",
    "        \"Momentum\": {momentum},\n",
    "        \"Batch_size\": {batch_size},\n",
    "        \"model version\": 0.0,\n",
    "        \"subset size [%]\": 100,\n",
    "        \"resize\": {subsize},\n",
    "        \"Normalized\": False,\n",
    "        \"Optimizer\": 'Adam',\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    run = train(train_dataloader, model, loss_fn, optimizer, run)\n",
    "    test_loss = test(val_dataloader, model, loss_fn)\n",
    "    run.log({\"Test_loss\": test_loss, \"Epoch\": t})\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5lsm0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
